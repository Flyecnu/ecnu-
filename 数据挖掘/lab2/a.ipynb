{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在上述代码中，我使用了多层感知机（Multilayer Perceptron，MLP）作为手写字符识别的模型。多层感知机是一种常见的前馈神经网络模型，它由多个全连接的隐藏层组成，用于学习输入数据之间的复杂非线性关系。在这个示例中，MLP模型由两个隐藏层构成，分别包含128个和64个神经元。\n",
    "\n",
    "多层感知机在手写字符识别等任务中具有良好的性能，能够从输入图像中提取有用的特征并进行分类。然而，MLP模型只是众多可能的模型之一，实际上还有许多其他的模型可以用于手写字符识别，如卷积神经网络（Convolutional Neural Network，CNN）等。\n",
    "\n",
    "根据您的具体需求和数据集特点，您可以选择不同的模型进行实验和比较，以找到最适合您任务的模型。不同模型可能在准确性、训练速度、模型复杂度等方面有所差异。因此，选择合适的模型需要综合考虑各种因素，并进行实验评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.9833333333333333\n",
      "Predictions saved to: 10205102417-predictions.txt\n"
     ]
    }
   ],
   "source": [
    " import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# 1. 数据加载和预处理\n",
    "def load_data(directory):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            label, _ = filename.split(\"_\")\n",
    "            with open(os.path.join(directory, filename), \"r\") as file:\n",
    "                image_data = file.read().replace('\\n', '')\n",
    "            image = np.array([int(pixel) for pixel in image_data])\n",
    "            images.append(image)\n",
    "            labels.append(int(label))\n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "train_images, train_labels = load_data(\"trainDigits\")  # 根据实际路径修改\n",
    "\n",
    "# 2. 拆分训练集和验证集\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_images, train_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# 3. 训练模型\n",
    "model = MLPClassifier(hidden_layer_sizes=(128, 64), max_iter=500)  # 使用多层感知机作为示例模型，可以根据需要选择其他算法\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 4. 在验证集上评估模型性能\n",
    "accuracy = model.score(X_val, y_val)\n",
    "print(\"Validation Accuracy:\", accuracy)\n",
    "\n",
    "# 5. 加载和预测测试数据\n",
    "def load_test_data(directory):\n",
    "    test_images = []\n",
    "    test_filenames = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            with open(os.path.join(directory, filename), \"r\") as file:\n",
    "                image_data = file.read().replace('\\n', '')\n",
    "            test_image = np.array([int(pixel) for pixel in image_data])\n",
    "            test_images.append(test_image)\n",
    "            test_filenames.append(filename.split(\".\")[0])\n",
    "    return np.array(test_images), test_filenames\n",
    "\n",
    "test_images, test_filenames = load_test_data(\"testfile_7\")  # 根据实际路径修改\n",
    "\n",
    "# 6. 进行预测并保存结果\n",
    "predictions = model.predict(test_images)\n",
    "sorted_indices = np.argsort(np.array(test_filenames, dtype=int))\n",
    "sorted_filenames = np.array(test_filenames)[sorted_indices]\n",
    "sorted_predictions = predictions[sorted_indices]\n",
    "\n",
    "output_filename = \"10205102417-predictions.txt\"  # 根据实际学号修改\n",
    "with open(output_filename, \"w\") as file:\n",
    "    for filename, prediction in zip(sorted_filenames, sorted_predictions):\n",
    "        file.write(str(filename) + \" \" + str(prediction) + \"\\n\")\n",
    "\n",
    "print(\"Predictions saved to:\", output_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhanghongwei/anaconda3/lib/python3.7/site-packages/sklearn/datasets/_openml.py:65: RuntimeWarning: Invalid cache, redownloading file\n",
      "  warn(\"Invalid cache, redownloading file\", RuntimeWarning)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Cannot interpret '<attribute 'dtype' of 'numpy.generic' objects>' as a data type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/datasets/_openml.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/datasets/_openml.py\u001b[0m in \u001b[0;36m_load_arff_response\u001b[0;34m(url, data_home, return_type, encode_nominal, parse_arff, md5_checksum)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 529\u001b[0;31m         \u001b[0mparsed_arff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_arff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/datasets/_openml.py\u001b[0m in \u001b[0;36m_convert_arff_data_dataframe\u001b[0;34m(arff, columns, features_dict)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m     \u001b[0mrow_bytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfirst_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory_usage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m     \u001b[0mchunksize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_chunk_n_rows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mmemory_usage\u001b[0;34m(self, index, deep)\u001b[0m\n\u001b[1;32m   2367\u001b[0m             result = Series(self.index.memory_usage(deep=deep),\n\u001b[0;32m-> 2368\u001b[0;31m                             index=['Index']).append(result)\n\u001b[0m\u001b[1;32m   2369\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[1;32m    274\u001b[0m                 data = _sanitize_array(data, index, dtype, copy,\n\u001b[0;32m--> 275\u001b[0;31m                                        raise_cast_failure=True)\n\u001b[0m\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_sanitize_array\u001b[0;34m(data, index, dtype, copy, raise_cast_failure)\u001b[0m\n\u001b[1;32m   4148\u001b[0m             subarr = construct_1d_arraylike_from_scalar(\n\u001b[0;32m-> 4149\u001b[0;31m                 value, len(index), dtype)\n\u001b[0m\u001b[1;32m   4150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/dtypes/cast.py\u001b[0m in \u001b[0;36mconstruct_1d_arraylike_from_scalar\u001b[0;34m(value, length, dtype)\u001b[0m\n\u001b[1;32m   1200\u001b[0m             \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1201\u001b[0;31m         \u001b[0msubarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1202\u001b[0m         \u001b[0msubarr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot interpret '<attribute 'dtype' of 'numpy.generic' objects>' as a data type",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-fcbd322f2669>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m# 2. 加载训练数据和MNIST数据集\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mtrain_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"trainDigits\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 根据实际路径修改\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mmnist_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmnist_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_mnist_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m# 3. 拆分训练集和验证集\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-fcbd322f2669>\u001b[0m in \u001b[0;36mload_mnist_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_mnist_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mmnist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfetch_openml\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mnist_784'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_home\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mnist_data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmnist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmnist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/datasets/_openml.py\u001b[0m in \u001b[0;36mfetch_openml\u001b[0;34m(name, version, data_id, data_home, target_column, cache, return_X_y, as_frame)\u001b[0m\n\u001b[1;32m    965\u001b[0m         \u001b[0mtarget_columns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_columns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m         \u001b[0mdata_columns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_columns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 967\u001b[0;31m         \u001b[0mmd5_checksum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_description\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"md5_checksum\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    968\u001b[0m     )\n\u001b[1;32m    969\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/datasets/_openml.py\u001b[0m in \u001b[0;36m_download_data_to_bunch\u001b[0;34m(url, sparse, data_home, as_frame, features_list, data_columns, target_columns, shape, md5_checksum)\u001b[0m\n\u001b[1;32m    659\u001b[0m         \u001b[0mencode_nominal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mas_frame\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    660\u001b[0m         \u001b[0mparse_arff\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_arff\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 661\u001b[0;31m         \u001b[0mmd5_checksum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmd5_checksum\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    662\u001b[0m     )\n\u001b[1;32m    663\u001b[0m     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnominal_attributes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpostprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/datasets/_openml.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munlink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/datasets/_openml.py\u001b[0m in \u001b[0;36m_load_arff_response\u001b[0;34m(url, data_home, return_type, encode_nominal, parse_arff, md5_checksum)\u001b[0m\n\u001b[1;32m    527\u001b[0m         )\n\u001b[1;32m    528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 529\u001b[0;31m         \u001b[0mparsed_arff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_arff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;31m# consume remaining stream, if early exited\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/datasets/_openml.py\u001b[0m in \u001b[0;36m_convert_arff_data_dataframe\u001b[0;34m(arff, columns, features_dict)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0mfirst_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfirst_row\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marff_columns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m     \u001b[0mrow_bytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfirst_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory_usage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m     \u001b[0mchunksize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_chunk_n_rows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mmemory_usage\u001b[0;34m(self, index, deep)\u001b[0m\n\u001b[1;32m   2366\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2367\u001b[0m             result = Series(self.index.memory_usage(deep=deep),\n\u001b[0;32m-> 2368\u001b[0;31m                             index=['Index']).append(result)\n\u001b[0m\u001b[1;32m   2369\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[1;32m    273\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m                 data = _sanitize_array(data, index, dtype, copy,\n\u001b[0;32m--> 275\u001b[0;31m                                        raise_cast_failure=True)\n\u001b[0m\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSingleBlockManager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfastpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_sanitize_array\u001b[0;34m(data, index, dtype, copy, raise_cast_failure)\u001b[0m\n\u001b[1;32m   4147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4148\u001b[0m             subarr = construct_1d_arraylike_from_scalar(\n\u001b[0;32m-> 4149\u001b[0;31m                 value, len(index), dtype)\n\u001b[0m\u001b[1;32m   4150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4151\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/dtypes/cast.py\u001b[0m in \u001b[0;36mconstruct_1d_arraylike_from_scalar\u001b[0;34m(value, length, dtype)\u001b[0m\n\u001b[1;32m   1199\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_integer_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1200\u001b[0m             \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1201\u001b[0;31m         \u001b[0msubarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1202\u001b[0m         \u001b[0msubarr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot interpret '<attribute 'dtype' of 'numpy.generic' objects>' as a data type"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "# 1. 数据加载和预处理\n",
    "def load_data(directory):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            label, _ = filename.split(\"_\")\n",
    "            with open(os.path.join(directory, filename), \"r\") as file:\n",
    "                image_data = file.read().replace('\\n', '')\n",
    "            image = np.array([int(pixel) for pixel in image_data])\n",
    "            images.append(image)\n",
    "            labels.append(int(label))\n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "def load_mnist_data():\n",
    "    mnist = fetch_openml('mnist_784', data_home='mnist_data')\n",
    "    X = mnist.data\n",
    "    y = mnist.target.astype(int)\n",
    "    return X, y\n",
    "\n",
    "# 2. 加载训练数据和MNIST数据集\n",
    "train_images, train_labels = load_data(\"trainDigits\")  # 根据实际路径修改\n",
    "mnist_images, mnist_labels = load_mnist_data()\n",
    "\n",
    "# 3. 拆分训练集和验证集\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_images, train_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# 4. 合并训练数据和MNIST数据集\n",
    "X_train = np.concatenate([X_train, mnist_images])\n",
    "y_train = np.concatenate([y_train, mnist_labels])\n",
    "\n",
    "# 5. 训练模型\n",
    "model = MLPClassifier(hidden_layer_sizes=(128, 64), max_iter=500)  # 使用多层感知机作为示例模型，可以根据需要选择其他算法\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 6. 在验证集上评估模型性能\n",
    "accuracy = model.score(X_val, y_val)\n",
    "print(\"Validation Accuracy:\", accuracy)\n",
    "\n",
    "# 7. 加载和预测测试数据\n",
    "def load_test_data(directory):\n",
    "    test_images = []\n",
    "    test_filenames = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            with open(os.path.join(directory, filename), \"r\") as file:\n",
    "                image_data = file.read().replace('\\n', '')\n",
    "            test_image = np.array([int(pixel) for pixel in image_data])\n",
    "            test_images.append(test_image)\n",
    "            test_filenames.append(filename.split(\".\")[0])\n",
    "    return np.array(test_images), test_filenames\n",
    "\n",
    "test_images, test_filenames = load_test_data(\"testfile_7\")  # 根据实际路径修改\n",
    "\n",
    "# 8. 进行预测并保存结果\n",
    "predictions = model.predict(test_images)\n",
    "sorted_indices = np.argsort(np.array(test_filenames, dtype=int))\n",
    "sorted_filenames = np.array(test_filenames)[sorted_indices]\n",
    "sorted_predictions = predictions[sorted_indices]\n",
    "\n",
    "output_filename = \"学号-predictions.txt\"  # 根据实际学号修改\n",
    "with open(output_filename, \"w\") as file:\n",
    "    for filename, prediction in zip(sorted_filenames, sorted_predictions):\n",
    "        file.write(str(filename) + \" \" + str(prediction) + \"\\n\")\n",
    "\n",
    "print(\"Predictions saved to:\", output_filename)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
