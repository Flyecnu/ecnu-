{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c74a9ba5-596a-461e-a655-82bfafeacf1e",
   "metadata": {},
   "source": [
    "## 5%数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35d365a0-91c5-4886-9d6d-242119bae08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import jieba\n",
    "import csv\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e738c97-072c-4915-ac5f-c69612c8bd04",
   "metadata": {},
   "source": [
    "## 载入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92a151b4-f1df-443f-b091-61ffa763fa5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open('Data/2%/test.txt','r',encoding='utf-8')\n",
    "f=open('Data/2%/2%.csv','a',newline='',encoding='utf-8')\n",
    "\n",
    "csv_writer = csv.writer(f)\n",
    "\n",
    "m=data.readlines()\n",
    "\n",
    "csv_writer.writerow([\"title\",\"tag\"])\n",
    "\n",
    "for i in m:\n",
    "    n=i.strip('\\n')\n",
    "    L=n.split('\\t')\n",
    "    # print(L)\n",
    "    csv_writer.writerow(L)\n",
    "    \n",
    "f.close()\n",
    "data.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8100e309-1117-40f5-99aa-2563d33bcc07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>今年一季度全国纪检监察机关处分省部级干部16人</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>习近平：携手迎接挑战 合作开创未来</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>瞭望丨引领人类海洋文明发展方向</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>广东省老区湾区同携手 接续奋进新时代</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>关注全民健身：健身场地少、消费高等难题怎么破？</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     title  tag\n",
       "0  今年一季度全国纪检监察机关处分省部级干部16人    0\n",
       "1       习近平：携手迎接挑战 合作开创未来     0\n",
       "2          瞭望丨引领人类海洋文明发展方向    0\n",
       "3       广东省老区湾区同携手 接续奋进新时代    0\n",
       "4  关注全民健身：健身场地少、消费高等难题怎么破？    0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.read_csv('Data/2%/2%.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58655cee-9191-4c17-a8d4-766b9b6b47c5",
   "metadata": {},
   "source": [
    "## 划分训练集测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd55aea9-ce63-45d4-a8a9-a1ebeda2bbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(data['title'], data['tag'], random_state=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7525546c-5a80-4a08-92f4-a56dd1dc7858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61\n",
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "print(x_test.shape[0])\n",
    "print(type(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c776a62-45af-4a9a-acf2-21d67b2b4ae4",
   "metadata": {},
   "source": [
    "## 分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a189285-b270-42a2-8541-412c74ceff3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\32852\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.584 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "241                         上联 ： 春夏秋冬 交 好运 ， 如何 对 下联 ？\n",
       "168             71 岁 施瓦辛格 心脏 手术 后 归来 ， 与 44 岁 女友 双双 现身\n",
       "78                      男子 打死 妹夫 潜逃 11 年   闹市 行医 逾 3 年\n",
       "47              萨达姆 究竟 多 有钱 ？ 看到 这些 照片 后 ， 网友 都 说 难以置信\n",
       "193    今日 直播 龙 星战 及 女子 围甲   +   星阵 让 先 9 连胜   +   暗渡陈仓\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fenci(train_data):\n",
    "    words_df = train_data.apply(lambda x:' '.join(jieba.cut(x)))\n",
    "    return words_df\n",
    "\n",
    "x_train_fenci = fenci(x_train)\n",
    "x_train_fenci[:5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59eaae1b-0766-4fcc-b5b2-9f5fe7e43cdc",
   "metadata": {},
   "source": [
    "## 停词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6afe0bb5-b981-4208-b3f0-3ce1179496ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = open(\"hit_stopwords.txt\",encoding='utf-8')\n",
    "stopwords_lst = infile.readlines()\n",
    "stopwords = [x.strip() for x in stopwords_lst]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beedbb5e-bf2c-415c-8a19-b5d9d84472d0",
   "metadata": {},
   "source": [
    "## 向量化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7dfd0cc-e8c1-4a6b-9166-fcc0afdaa1ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Software\\Anaconda\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['lex', '①①', '①②', '①③', '①④', '①⑤', '①⑥', '①⑦', '①⑧', '①⑨', '①ａ', '①ｂ', '①ｃ', '①ｄ', '①ｅ', '①ｆ', '①ｇ', '①ｈ', '①ｉ', '①ｏ', '②①', '②②', '②③', '②④', '②⑤', '②⑥', '②⑦', '②⑧', '②⑩', '②ａ', '②ｂ', '②ｄ', '②ｅ', '②ｆ', '②ｇ', '②ｈ', '②ｉ', '②ｊ', '③①', '③⑩', '③ａ', '③ｂ', '③ｃ', '③ｄ', '③ｅ', '③ｆ', '③ｇ', '③ｈ', '④ａ', '④ｂ', '④ｃ', '④ｄ', '④ｅ', '⑤ａ', '⑤ｂ', '⑤ｄ', '⑤ｅ', '⑤ｆ', '１２', 'ｌｉ', 'ｚｘｆｉｔｌ'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CountVectorizer(max_features=5000,\n",
       "                stop_words=['———', '》），', '）÷（１－', '”，', '）、', '＝（', ':', '→',\n",
       "                            '℃', '&', '*', '一一', '~~~~', '’', '.', '『', '.一',\n",
       "                            './', '--', '』', '＝″', '【', '［＊］', '｝＞', '［⑤］］',\n",
       "                            '［①Ｄ］', 'ｃ］', 'ｎｇ昉', '＊', '//', ...])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words=stopwords, max_features=5000)\n",
    "vectorizer.fit(x_train_fenci)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a1759af-ddcc-4572-9637-4348f2fb1aa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.39344262295081966"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "classifier = MultinomialNB()\n",
    "#模型训练\n",
    "classifier.fit(vectorizer.transform(x_train_fenci), y_train)\n",
    "#使用训练好的模型进行预测\n",
    "classifier.score(vectorizer.transform(fenci(x_test)), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c3def4-b30f-45bc-92b3-a3ede9750d93",
   "metadata": {},
   "source": [
    "## 预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5fd54599-f752-4f5d-b563-1f9506e66c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "每个类别的精确率与召回率\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.40      0.36         5\n",
      "           1       0.55      0.75      0.63         8\n",
      "           2       0.40      1.00      0.57         2\n",
      "           3       0.14      1.00      0.25         1\n",
      "           4       0.71      0.62      0.67         8\n",
      "           5       1.00      0.09      0.16        23\n",
      "           6       0.12      0.50      0.20         2\n",
      "           7       0.50      0.33      0.40         9\n",
      "           8       0.22      0.67      0.33         3\n",
      "\n",
      "    accuracy                           0.39        61\n",
      "   macro avg       0.44      0.60      0.40        61\n",
      "weighted avg       0.67      0.39      0.37        61\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "predict=classifier.predict(vectorizer.transform(fenci(x_test)))\n",
    "print(\"每个类别的精确率与召回率\\n\",classification_report(predict, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d0ff0f59-7c46-4520-855e-f8e425156ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open('test_5.txt','r',encoding='utf-8')\n",
    "\n",
    "L=[]\n",
    "m=f.readlines()\n",
    "for i in m:\n",
    "    n=i.strip('\\n')\n",
    "    x=n.split('\\t')\n",
    "    L.append(x[1])\n",
    "\n",
    "data_to_predict=pd.Series(L)\n",
    "# data_to_predict=np.matrix(data_to_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff4cb344-b128-4cf1-9509-f9651f947fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(603,)\n"
     ]
    }
   ],
   "source": [
    "print(data_to_predict.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "77a738a8-c314-4860-9fb0-374288c93ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict=classifier.predict(vectorizer.transform(fenci(data_to_predict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d23fd5f4-569c-41b2-b0ae-fe49558fb5c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 7 2 5 3 2 5 0 5 7 2 0 4 7 0 4 5 5 7 0 5 2 5 6 4 4 5 4 5 2 0 4 2 8 0 5 5\n",
      " 5 5 5 5 7 6 5 5 0 4 2 0 1 5 5 5 7 4 5 5 5 2 2 0 5 1 1 5 0 2 6 4 0 6 5 1 0\n",
      " 5 8 5 5 2 3 5 4 4 5 4 1 5 5 5 5 4 3 5 5 5 5 3 5 3 5 5 5 5 4 4 5 1 3 7 8 3\n",
      " 2 5 5 3 5 4 5 5 5 0 1 5 7 5 5 1 4 5 5 1 4 4 1 7 6 5 3 0 2 1 5 5 7 0 5 7 1\n",
      " 5 4 4 5 5 0 0 1 5 7 0 1 5 6 0 5 2 0 5 2 3 4 5 6 5 5 1 5 4 0 4 5 7 5 4 5 2\n",
      " 5 7 0 5 5 5 5 5 2 3 2 2 4 7 6 7 5 7 7 0 4 0 5 5 2 5 2 2 6 4 2 0 4 5 7 0 2\n",
      " 5 7 0 2 2 5 2 5 5 0 5 5 0 7 6 5 1 5 4 7 5 0 0 4 5 5 7 5 7 2 5 5 0 7 5 3 5\n",
      " 5 5 5 0 2 1 2 4 5 5 5 2 4 0 0 4 4 4 1 7 0 2 4 4 5 0 8 5 0 2 0 0 4 2 5 5 2\n",
      " 5 1 7 0 5 8 5 0 2 5 5 5 5 4 5 0 2 5 6 5 7 0 5 6 8 0 4 5 5 2 5 3 5 5 4 5 2\n",
      " 5 4 5 5 7 5 0 5 5 2 2 5 1 4 0 6 2 0 5 6 0 3 2 4 0 1 2 5 7 5 8 5 5 5 0 5 5\n",
      " 4 5 4 5 1 2 5 6 2 5 2 4 3 5 5 0 0 4 6 0 4 5 1 7 5 1 5 4 6 2 4 5 4 5 1 0 5\n",
      " 0 4 5 5 0 5 2 5 0 5 7 5 1 4 0 7 1 5 5 4 5 5 1 5 5 5 5 7 5 5 5 4 2 5 5 5 6\n",
      " 5 2 5 2 7 5 0 5 4 5 4 5 5 5 0 1 0 5 6 2 4 0 5 5 6 5 0 5 0 4 3 5 5 5 5 5 2\n",
      " 2 0 7 5 7 5 2 0 7 1 5 0 5 5 6 2 5 5 0 5 2 2 1 5 1 2 5 2 7 5 4 5 5 5 0 5 5\n",
      " 0 3 5 0 0 8 2 6 5 5 5 0 0 5 3 2 5 5 5 5 0 5 7 5 1 6 8 0 8 0 4 5 1 0 4 0 5\n",
      " 3 0 4 5 7 1 5 7 3 5 5 7 4 5 8 1 5 4 6 5 0 5 5 1 5 3 2 3 0 0 5 5 1 5 0 5 4\n",
      " 5 0 4 4 6 0 5 5 5 2 0]\n"
     ]
    }
   ],
   "source": [
    "print(predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257ff2c0-f563-4ffd-8cb7-92f3344b7358",
   "metadata": {},
   "source": [
    "## TF-IDF预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "76315037-6bca-4703-ad4a-c57dbe275364",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Software\\Anaconda\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['①①', '①②', '①③', '①④', '①⑤', '①⑥', '①⑦', '①⑧', '①⑨', '①Ａ', '①Ｂ', '①Ｃ', '①Ｄ', '①Ｅ', '①ａ', '①ｃ', '①ｄ', '①ｅ', '①ｆ', '①ｇ', '①ｈ', '①ｉ', '①ｏ', '②①', '②②', '②③', '②④', '②⑤', '②⑥', '②⑦', '②⑧', '②⑩', '②Ｂ', '②Ｇ', '②ａ', '②ｂ', '②ｄ', '②ｅ', '②ｆ', '②ｇ', '②ｈ', '②ｉ', '②ｊ', '③①', '③⑩', '③Ｆ', '③ａ', '③ｂ', '③ｃ', '③ｄ', '③ｅ', '③ｇ', '③ｈ', '④ａ', '④ｂ', '④ｃ', '④ｄ', '④ｅ', '⑤ａ', '⑤ｂ', '⑤ｄ', '⑤ｅ', '⑤ｆ', '１２'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.39344262295081966"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#使用tf-idf把文本转为向量\n",
    "tv = TfidfVectorizer(stop_words=stopwords, max_features=5000, lowercase = False)\n",
    "tv.fit(x_train_fenci)\n",
    "#模型训练\n",
    "classifier.fit(tv.transform(fenci(x_train)), y_train)\n",
    "#利用训练好的模型测试\n",
    "classifier.score(tv.transform(fenci(x_test)), y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "174fd17d-da73-4d38-a1d0-c14fcdfcdd66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "每个类别的精确率与召回率\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.40      0.36         5\n",
      "           1       0.55      0.75      0.63         8\n",
      "           2       0.40      1.00      0.57         2\n",
      "           3       0.14      1.00      0.25         1\n",
      "           4       0.71      0.62      0.67         8\n",
      "           5       1.00      0.08      0.15        24\n",
      "           6       0.12      0.50      0.20         2\n",
      "           7       0.50      0.33      0.40         9\n",
      "           8       0.22      1.00      0.36         2\n",
      "\n",
      "    accuracy                           0.39        61\n",
      "   macro avg       0.44      0.63      0.40        61\n",
      "weighted avg       0.69      0.39      0.36        61\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "predict=classifier.predict(tv.transform(fenci(x_test)))\n",
    "print(\"每个类别的精确率与召回率\\n\",classification_report(predict, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fbae6c8-44ef-473c-8399-a85292516a2e",
   "metadata": {},
   "source": [
    "## N-gram 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1ae0e946-6426-4e82-b37d-2c17258821c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Software\\Anaconda\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['①①', '①②', '①③', '①④', '①⑤', '①⑥', '①⑦', '①⑧', '①⑨', '①Ａ', '①Ｂ', '①Ｃ', '①Ｄ', '①Ｅ', '①ａ', '①ｃ', '①ｄ', '①ｅ', '①ｆ', '①ｇ', '①ｈ', '①ｉ', '①ｏ', '②①', '②②', '②③', '②④', '②⑤', '②⑥', '②⑦', '②⑧', '②⑩', '②Ｂ', '②Ｇ', '②ａ', '②ｂ', '②ｄ', '②ｅ', '②ｆ', '②ｇ', '②ｈ', '②ｉ', '②ｊ', '③①', '③⑩', '③Ｆ', '③ａ', '③ｂ', '③ｃ', '③ｄ', '③ｅ', '③ｇ', '③ｈ', '④ａ', '④ｂ', '④ｃ', '④ｄ', '④ｅ', '⑤ａ', '⑤ｂ', '⑤ｄ', '⑤ｅ', '⑤ｆ', '１２'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.3442622950819672"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tv_2gram = TfidfVectorizer(stop_words=stopwords, max_features=5000, ngram_range=(1,2),lowercase = False)\n",
    "tv_2gram.fit(x_train_fenci)\n",
    "#训练模型\n",
    "clf_2gram = MultinomialNB()\n",
    "clf_2gram.fit(tv_2gram.transform(fenci(x_train)), y_train)\n",
    "#预测\n",
    "clf_2gram.score(tv_2gram.transform(fenci(x_test)), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c28c78ba-57c2-4207-9d97-90f50e8bc0a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "每个类别的精确率与召回率\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.40      0.36         5\n",
      "           1       0.55      0.75      0.63         8\n",
      "           2       0.40      1.00      0.57         2\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.57      0.67      0.62         6\n",
      "           5       1.00      0.07      0.13        29\n",
      "           6       0.00      0.00      0.00         0\n",
      "           7       0.50      0.33      0.40         9\n",
      "           8       0.22      1.00      0.36         2\n",
      "\n",
      "    accuracy                           0.34        61\n",
      "   macro avg       0.40      0.47      0.34        61\n",
      "weighted avg       0.72      0.34      0.32        61\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Software\\Anaconda\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "D:\\Software\\Anaconda\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "D:\\Software\\Anaconda\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "predict=clf_2gram.predict(tv_2gram.transform(fenci(x_test)))\n",
    "print(\"每个类别的精确率与召回率\\n\",classification_report(predict, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148d1be2-0413-427a-a005-16ec95209401",
   "metadata": {},
   "source": [
    "## 3-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fd6dc1bd-7b71-4d90-baf5-159284dd54c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Software\\Anaconda\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['①①', '①②', '①③', '①④', '①⑤', '①⑥', '①⑦', '①⑧', '①⑨', '①Ａ', '①Ｂ', '①Ｃ', '①Ｄ', '①Ｅ', '①ａ', '①ｃ', '①ｄ', '①ｅ', '①ｆ', '①ｇ', '①ｈ', '①ｉ', '①ｏ', '②①', '②②', '②③', '②④', '②⑤', '②⑥', '②⑦', '②⑧', '②⑩', '②Ｂ', '②Ｇ', '②ａ', '②ｂ', '②ｄ', '②ｅ', '②ｆ', '②ｇ', '②ｈ', '②ｉ', '②ｊ', '③①', '③⑩', '③Ｆ', '③ａ', '③ｂ', '③ｃ', '③ｄ', '③ｅ', '③ｇ', '③ｈ', '④ａ', '④ｂ', '④ｃ', '④ｄ', '④ｅ', '⑤ａ', '⑤ｂ', '⑤ｄ', '⑤ｅ', '⑤ｆ', '１２'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.3442622950819672"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tv_3gram = TfidfVectorizer(stop_words=stopwords, max_features=5000, ngram_range=(1,3),lowercase = False)\n",
    "tv_3gram.fit(x_train_fenci)\n",
    "#训练模型\n",
    "clf_3gram = MultinomialNB()\n",
    "clf_3gram.fit(tv_3gram.transform(fenci(x_train)), y_train)\n",
    "#预测\n",
    "clf_3gram.score(tv_3gram.transform(fenci(x_test)), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a677781f-da97-4706-9e25-d4aa66a88f24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "每个类别的精确率与召回率\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.40      0.36         5\n",
      "           1       0.55      0.75      0.63         8\n",
      "           2       0.40      1.00      0.57         2\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.57      0.80      0.67         5\n",
      "           5       1.00      0.06      0.12        31\n",
      "           6       0.00      0.00      0.00         0\n",
      "           7       0.50      0.38      0.43         8\n",
      "           8       0.22      1.00      0.36         2\n",
      "\n",
      "    accuracy                           0.34        61\n",
      "   macro avg       0.40      0.49      0.35        61\n",
      "weighted avg       0.74      0.34      0.32        61\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Software\\Anaconda\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "D:\\Software\\Anaconda\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "D:\\Software\\Anaconda\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "predict=clf_3gram.predict(tv_3gram.transform(fenci(x_test)))\n",
    "print(\"每个类别的精确率与召回率\\n\",classification_report(predict, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493c8360-1db2-4f32-8f49-f7f5d906cb40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
