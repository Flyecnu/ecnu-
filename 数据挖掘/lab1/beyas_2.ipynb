{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c74a9ba5-596a-461e-a655-82bfafeacf1e",
   "metadata": {},
   "source": [
    "## 朴素贝叶斯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35d365a0-91c5-4886-9d6d-242119bae08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import jieba\n",
    "import csv\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e738c97-072c-4915-ac5f-c69612c8bd04",
   "metadata": {},
   "source": [
    "## 载入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92a151b4-f1df-443f-b091-61ffa763fa5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open('Data/data_new/test.txt','r',encoding='utf-8')\n",
    "f=open('Data/data_new/4.csv','a',newline='',encoding='utf-8')\n",
    "\n",
    "csv_writer = csv.writer(f)\n",
    "\n",
    "m=data.readlines()\n",
    "\n",
    "csv_writer.writerow([\"title\",\"tag\"])\n",
    "\n",
    "for i in m:\n",
    "    n=i.strip('\\n')\n",
    "    L=n.split('\\t')\n",
    "    # print(L)\n",
    "    csv_writer.writerow(L)\n",
    "    \n",
    "f.close()\n",
    "data.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8100e309-1117-40f5-99aa-2563d33bcc07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>今年一季度全国纪检监察机关处分省部级干部16人</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>习近平：携手迎接挑战 合作开创未来</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>瞭望丨引领人类海洋文明发展方向</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>广东省老区湾区同携手 接续奋进新时代</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>关注全民健身：健身场地少、消费高等难题怎么破？</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     title  tag\n",
       "0  今年一季度全国纪检监察机关处分省部级干部16人    0\n",
       "1       习近平：携手迎接挑战 合作开创未来     0\n",
       "2          瞭望丨引领人类海洋文明发展方向    0\n",
       "3       广东省老区湾区同携手 接续奋进新时代    0\n",
       "4  关注全民健身：健身场地少、消费高等难题怎么破？    0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.read_csv('Data/data_new/4.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58655cee-9191-4c17-a8d4-766b9b6b47c5",
   "metadata": {},
   "source": [
    "## 划分训练集测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd55aea9-ce63-45d4-a8a9-a1ebeda2bbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(data['title'], data['tag'], random_state=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7525546c-5a80-4a08-92f4-a56dd1dc7858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8735\n",
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "print(x_test.shape[0])\n",
    "print(type(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c776a62-45af-4a9a-acf2-21d67b2b4ae4",
   "metadata": {},
   "source": [
    "## 分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a189285-b270-42a2-8541-412c74ceff3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\32852\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.635 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "33275                                  易经 — （ 月 象图 ）\n",
       "25108               王思聪 现身 防疫站 打 疫苗 ， 王 可可 看来 要 失宠 了\n",
       "30563    英格兰 的 新 金童   能 从 普通 球星 上升 到 梅西 这样 的 超级 巨星 吗\n",
       "18379              无证 驾驶 ， 造成 人身 伤亡 ， 交强险 是否 也 不 赔 ？\n",
       "26937          为国争光 ！ 中国乒乓球队 凯旋归来   球迷 热情 迎接   大满贯 ！\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fenci(train_data):\n",
    "    words_df = train_data.apply(lambda x:' '.join(jieba.cut(x)))\n",
    "    return words_df\n",
    "\n",
    "x_train_fenci = fenci(x_train)\n",
    "x_train_fenci[:5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59eaae1b-0766-4fcc-b5b2-9f5fe7e43cdc",
   "metadata": {},
   "source": [
    "## 停词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6afe0bb5-b981-4208-b3f0-3ce1179496ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = open(\"hit_stopwords.txt\",encoding='utf-8')\n",
    "stopwords_lst = infile.readlines()\n",
    "stopwords = [x.strip() for x in stopwords_lst]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beedbb5e-bf2c-415c-8a19-b5d9d84472d0",
   "metadata": {},
   "source": [
    "## 向量化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b7dfd0cc-e8c1-4a6b-9166-fcc0afdaa1ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Software\\Anaconda\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['lex', '①①', '①②', '①③', '①④', '①⑤', '①⑥', '①⑦', '①⑧', '①⑨', '①ａ', '①ｂ', '①ｃ', '①ｄ', '①ｅ', '①ｆ', '①ｇ', '①ｈ', '①ｉ', '①ｏ', '②①', '②②', '②③', '②④', '②⑤', '②⑥', '②⑦', '②⑧', '②⑩', '②ａ', '②ｂ', '②ｄ', '②ｅ', '②ｆ', '②ｇ', '②ｈ', '②ｉ', '②ｊ', '③①', '③⑩', '③ａ', '③ｂ', '③ｃ', '③ｄ', '③ｅ', '③ｆ', '③ｇ', '③ｈ', '④ａ', '④ｂ', '④ｃ', '④ｄ', '④ｅ', '⑤ａ', '⑤ｂ', '⑤ｄ', '⑤ｅ', '⑤ｆ', '１２', 'ｌｉ', 'ｚｘｆｉｔｌ'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CountVectorizer(max_features=5000,\n",
       "                stop_words=['———', '》），', '）÷（１－', '”，', '）、', '＝（', ':', '→',\n",
       "                            '℃', '&', '*', '一一', '~~~~', '’', '.', '『', '.一',\n",
       "                            './', '--', '』', '＝″', '【', '［＊］', '｝＞', '［⑤］］',\n",
       "                            '［①Ｄ］', 'ｃ］', 'ｎｇ昉', '＊', '//', ...])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words=stopwords, max_features=5000)\n",
    "vectorizer.fit(x_train_fenci)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6a1759af-ddcc-4572-9637-4348f2fb1aa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8022896393817973"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "classifier = MultinomialNB()\n",
    "#模型训练\n",
    "classifier.fit(vectorizer.transform(x_train_fenci), y_train)\n",
    "#使用训练好的模型进行预测\n",
    "classifier.score(vectorizer.transform(fenci(x_test)), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c3def4-b30f-45bc-92b3-a3ede9750d93",
   "metadata": {},
   "source": [
    "## 预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5fd54599-f752-4f5d-b563-1f9506e66c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "每个类别的精确率与召回率               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.75      0.66       444\n",
      "           1       0.74      0.79      0.76       864\n",
      "           2       0.77      0.69      0.73       645\n",
      "           3       0.82      0.79      0.81       940\n",
      "           4       0.86      0.85      0.86      1155\n",
      "           5       0.86      0.85      0.86      1219\n",
      "           6       0.84      0.71      0.77      1537\n",
      "           7       0.88      0.88      0.88      1229\n",
      "           8       0.69      0.87      0.77       702\n",
      "\n",
      "    accuracy                           0.80      8735\n",
      "   macro avg       0.78      0.80      0.79      8735\n",
      "weighted avg       0.81      0.80      0.80      8735\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "predict=classifier.predict(vectorizer.transform(fenci(x_test)))\n",
    "print(\"每个类别的精确率与召回率\",classification_report(predict, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d0ff0f59-7c46-4520-855e-f8e425156ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open('test_5.txt','r',encoding='utf-8')\n",
    "\n",
    "L=[]\n",
    "m=f.readlines()\n",
    "for i in m:\n",
    "    n=i.strip('\\n')\n",
    "    x=n.split('\\t')\n",
    "    L.append(x[1])\n",
    "\n",
    "data_to_predict=pd.Series(L)\n",
    "# data_to_predict=np.matrix(data_to_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ff4cb344-b128-4cf1-9509-f9651f947fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(603,)\n"
     ]
    }
   ],
   "source": [
    "print(data_to_predict.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "77a738a8-c314-4860-9fb0-374288c93ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict=classifier.predict(vectorizer.transform(fenci(data_to_predict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d23fd5f4-569c-41b2-b0ae-fe49558fb5c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 4 1 6 3 6 6 3 6 6 7 3 4 7 3 0 5 5 1 5 0 4 7 8 4 5 3 6 3 7 1 2 0 4 3 5 4\n",
      " 6 3 3 5 3 1 4 5 7 4 0 1 3 5 0 5 7 7 8 0 2 2 0 3 6 3 3 3 0 6 6 2 0 6 3 1 4\n",
      " 0 8 4 5 1 3 0 5 4 3 6 0 4 3 3 0 2 4 5 6 5 0 3 7 3 5 5 5 7 4 4 1 3 7 5 0 4\n",
      " 6 7 3 3 5 4 3 6 6 0 1 5 1 7 7 0 6 5 1 1 4 4 1 3 5 8 0 0 2 7 3 2 1 5 4 0 1\n",
      " 1 3 4 1 6 3 0 1 0 7 0 1 6 6 3 5 2 0 0 6 4 1 4 0 6 6 7 5 4 3 2 3 4 1 4 3 1\n",
      " 6 6 6 5 4 3 6 6 5 7 1 2 4 8 0 1 3 7 3 1 0 8 5 8 2 7 5 2 6 4 7 0 4 4 7 8 2\n",
      " 5 2 3 2 4 4 3 3 6 0 5 1 0 6 6 6 3 0 4 6 0 4 2 4 1 0 2 3 7 2 3 3 7 6 7 7 6\n",
      " 0 3 5 3 0 8 4 3 7 7 8 3 4 1 5 6 2 4 6 7 3 2 4 4 5 1 7 7 3 2 2 8 4 6 7 3 0\n",
      " 4 8 5 1 3 8 0 0 8 5 0 8 5 8 7 8 2 4 0 2 6 0 0 4 6 4 4 3 5 6 6 4 0 0 6 0 2\n",
      " 8 3 0 0 7 2 4 5 6 2 4 6 1 4 7 6 2 0 7 7 4 6 0 4 3 1 2 0 3 4 8 0 5 8 1 5 6\n",
      " 4 3 6 5 1 2 5 1 6 5 2 4 3 5 4 1 0 0 6 7 4 5 1 6 5 4 6 4 6 1 3 0 3 7 7 3 5\n",
      " 4 4 3 5 1 5 5 0 0 5 7 0 3 7 3 7 1 0 7 4 5 3 0 5 6 7 5 7 0 5 2 2 1 5 8 0 4\n",
      " 4 2 5 3 4 7 3 5 4 7 4 5 3 6 3 1 8 7 3 2 4 3 3 3 6 3 1 4 7 0 5 6 5 1 1 4 2\n",
      " 2 3 7 0 6 7 2 0 3 3 4 0 2 7 8 2 0 5 0 2 2 2 0 0 1 6 2 0 6 7 2 3 6 5 8 8 5\n",
      " 4 4 6 3 8 6 2 4 2 6 0 0 1 5 3 2 0 2 6 5 3 3 7 3 1 7 4 8 2 1 4 4 1 0 3 7 5\n",
      " 0 3 2 2 3 1 1 7 3 5 7 3 3 1 1 1 4 4 6 5 1 6 4 1 0 3 2 4 3 3 3 6 8 2 0 3 4\n",
      " 5 7 3 7 8 0 2 0 4 8 2]\n"
     ]
    }
   ],
   "source": [
    "print(predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257ff2c0-f563-4ffd-8cb7-92f3344b7358",
   "metadata": {},
   "source": [
    "## TF-IDF预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "76315037-6bca-4703-ad4a-c57dbe275364",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Software\\Anaconda\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['①①', '①②', '①③', '①④', '①⑤', '①⑥', '①⑦', '①⑧', '①⑨', '①Ａ', '①Ｂ', '①Ｃ', '①Ｄ', '①Ｅ', '①ａ', '①ｃ', '①ｄ', '①ｅ', '①ｆ', '①ｇ', '①ｈ', '①ｉ', '①ｏ', '②①', '②②', '②③', '②④', '②⑤', '②⑥', '②⑦', '②⑧', '②⑩', '②Ｂ', '②Ｇ', '②ａ', '②ｂ', '②ｄ', '②ｅ', '②ｆ', '②ｇ', '②ｈ', '②ｉ', '②ｊ', '③①', '③⑩', '③Ｆ', '③ａ', '③ｂ', '③ｃ', '③ｄ', '③ｅ', '③ｇ', '③ｈ', '④ａ', '④ｂ', '④ｃ', '④ｄ', '④ｅ', '⑤ａ', '⑤ｂ', '⑤ｄ', '⑤ｅ', '⑤ｆ', '１２'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7978248425872925"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#使用tf-idf把文本转为向量\n",
    "tv = TfidfVectorizer(stop_words=stopwords, max_features=5000, lowercase = False)\n",
    "tv.fit(x_train_fenci)\n",
    "#模型训练\n",
    "classifier.fit(tv.transform(fenci(x_train)), y_train)\n",
    "#利用训练好的模型测试\n",
    "classifier.score(tv.transform(fenci(x_test)), y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fbae6c8-44ef-473c-8399-a85292516a2e",
   "metadata": {},
   "source": [
    "## N-gram 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1ae0e946-6426-4e82-b37d-2c17258821c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Software\\Anaconda\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['①①', '①②', '①③', '①④', '①⑤', '①⑥', '①⑦', '①⑧', '①⑨', '①Ａ', '①Ｂ', '①Ｃ', '①Ｄ', '①Ｅ', '①ａ', '①ｃ', '①ｄ', '①ｅ', '①ｆ', '①ｇ', '①ｈ', '①ｉ', '①ｏ', '②①', '②②', '②③', '②④', '②⑤', '②⑥', '②⑦', '②⑧', '②⑩', '②Ｂ', '②Ｇ', '②ａ', '②ｂ', '②ｄ', '②ｅ', '②ｆ', '②ｇ', '②ｈ', '②ｉ', '②ｊ', '③①', '③⑩', '③Ｆ', '③ａ', '③ｂ', '③ｃ', '③ｄ', '③ｅ', '③ｇ', '③ｈ', '④ａ', '④ｂ', '④ｃ', '④ｄ', '④ｅ', '⑤ａ', '⑤ｂ', '⑤ｄ', '⑤ｅ', '⑤ｆ', '１２'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7935890097309674"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tv_2gram = TfidfVectorizer(stop_words=stopwords, max_features=5000, ngram_range=(1,2),lowercase = False)\n",
    "tv_2gram.fit(x_train_fenci)\n",
    "#训练模型\n",
    "clf_2gram = MultinomialNB()\n",
    "clf_2gram.fit(tv_2gram.transform(fenci(x_train)), y_train)\n",
    "#预测\n",
    "clf_2gram.score(tv_2gram.transform(fenci(x_test)), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28c78ba-57c2-4207-9d97-90f50e8bc0a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
