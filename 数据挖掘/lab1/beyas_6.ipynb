{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c74a9ba5-596a-461e-a655-82bfafeacf1e",
   "metadata": {},
   "source": [
    "## 50%数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35d365a0-91c5-4886-9d6d-242119bae08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import jieba\n",
    "import csv\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e738c97-072c-4915-ac5f-c69612c8bd04",
   "metadata": {},
   "source": [
    "## 载入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "92a151b4-f1df-443f-b091-61ffa763fa5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open('Data/50%/train.txt','r',encoding='utf-8')\n",
    "f=open('Data/50%/50%.csv','a',newline='',encoding='utf-8')\n",
    "\n",
    "csv_writer = csv.writer(f)\n",
    "\n",
    "m=data.readlines()\n",
    "\n",
    "csv_writer.writerow([\"title\",\"tag\"])\n",
    "\n",
    "for i in m:\n",
    "    n=i.strip('\\n')\n",
    "    L=n.split('\\t')\n",
    "    # print(L)\n",
    "    csv_writer.writerow(L)\n",
    "    \n",
    "f.close()\n",
    "data.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8100e309-1117-40f5-99aa-2563d33bcc07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>生态环境部：一季度全国生态环境质量总体持续改善</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>科技战“疫”，全球寻何新“利器”？</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>种子方舟：国家农作物种质资源库</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>新《地名管理条例》：开启我国地名管理的新时代</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>新时代的中国青年： 生逢中华民族发展最好时期</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     title  tag\n",
       "0  生态环境部：一季度全国生态环境质量总体持续改善    0\n",
       "1        科技战“疫”，全球寻何新“利器”？    0\n",
       "2          种子方舟：国家农作物种质资源库    0\n",
       "3   新《地名管理条例》：开启我国地名管理的新时代    0\n",
       "4   新时代的中国青年： 生逢中华民族发展最好时期    0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.read_csv('Data/50%/50%.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58655cee-9191-4c17-a8d4-766b9b6b47c5",
   "metadata": {},
   "source": [
    "## 划分训练集测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cd55aea9-ce63-45d4-a8a9-a1ebeda2bbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(data['title'], data['tag'], random_state=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7525546c-5a80-4a08-92f4-a56dd1dc7858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21860\n",
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "print(x_test.shape[0])\n",
    "print(type(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c776a62-45af-4a9a-acf2-21d67b2b4ae4",
   "metadata": {},
   "source": [
    "## 分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0a189285-b270-42a2-8541-412c74ceff3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52110    奔驰 EQ   C 电动 SUV 谍 照曝光   对手 锁定 奥迪 e - tron   Q...\n",
       "9537         一触即发 ？ 俄罗斯 一 超级 武器 首次 曝光 ， 美国 以后 只有 挨打 的 份儿 ？\n",
       "8650               超级 赌徒 竟 拿 家人 当 赌注 ， 输掉 赌局 后 妻子 改嫁 儿子 改性\n",
       "39893                   30 多名 台湾 诈骗犯 亚美尼亚 被控   台媒 : 大陆 没 抢\n",
       "84206                          上联 ： 遍地 鲜花 摇 落地 ， 如何 对 下联 ？\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fenci(train_data):\n",
    "    words_df = train_data.apply(lambda x:' '.join(jieba.cut(x)))\n",
    "    return words_df\n",
    "\n",
    "x_train_fenci = fenci(x_train)\n",
    "x_train_fenci[:5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59eaae1b-0766-4fcc-b5b2-9f5fe7e43cdc",
   "metadata": {},
   "source": [
    "## 停词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6afe0bb5-b981-4208-b3f0-3ce1179496ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = open(\"hit_stopwords.txt\",encoding='utf-8')\n",
    "stopwords_lst = infile.readlines()\n",
    "stopwords = [x.strip() for x in stopwords_lst]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beedbb5e-bf2c-415c-8a19-b5d9d84472d0",
   "metadata": {},
   "source": [
    "## 向量化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b7dfd0cc-e8c1-4a6b-9166-fcc0afdaa1ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Software\\Anaconda\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['lex', '①①', '①②', '①③', '①④', '①⑤', '①⑥', '①⑦', '①⑧', '①⑨', '①ａ', '①ｂ', '①ｃ', '①ｄ', '①ｅ', '①ｆ', '①ｇ', '①ｈ', '①ｉ', '①ｏ', '②①', '②②', '②③', '②④', '②⑤', '②⑥', '②⑦', '②⑧', '②⑩', '②ａ', '②ｂ', '②ｄ', '②ｅ', '②ｆ', '②ｇ', '②ｈ', '②ｉ', '②ｊ', '③①', '③⑩', '③ａ', '③ｂ', '③ｃ', '③ｄ', '③ｅ', '③ｆ', '③ｇ', '③ｈ', '④ａ', '④ｂ', '④ｃ', '④ｄ', '④ｅ', '⑤ａ', '⑤ｂ', '⑤ｄ', '⑤ｅ', '⑤ｆ', '１２', 'ｌｉ', 'ｚｘｆｉｔｌ'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CountVectorizer(max_features=5000,\n",
       "                stop_words=['———', '》），', '）÷（１－', '”，', '）、', '＝（', ':', '→',\n",
       "                            '℃', '&', '*', '一一', '~~~~', '’', '.', '『', '.一',\n",
       "                            './', '--', '』', '＝″', '【', '［＊］', '｝＞', '［⑤］］',\n",
       "                            '［①Ｄ］', 'ｃ］', 'ｎｇ昉', '＊', '//', ...])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words=stopwords, max_features=5000)\n",
    "vectorizer.fit(x_train_fenci)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6a1759af-ddcc-4572-9637-4348f2fb1aa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8222323879231473"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "classifier = MultinomialNB()\n",
    "#模型训练\n",
    "classifier.fit(vectorizer.transform(x_train_fenci), y_train)\n",
    "#使用训练好的模型进行预测\n",
    "classifier.score(vectorizer.transform(fenci(x_test)), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c3def4-b30f-45bc-92b3-a3ede9750d93",
   "metadata": {},
   "source": [
    "## 预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5fd54599-f752-4f5d-b563-1f9506e66c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "每个类别的精确率与召回率\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.76      0.71      1200\n",
      "           1       0.76      0.79      0.78      2110\n",
      "           2       0.82      0.75      0.78      1723\n",
      "           3       0.81      0.79      0.80      2360\n",
      "           4       0.86      0.87      0.87      2669\n",
      "           5       0.88      0.87      0.88      2999\n",
      "           6       0.87      0.75      0.81      3828\n",
      "           7       0.88      0.91      0.89      2971\n",
      "           8       0.73      0.85      0.79      2000\n",
      "\n",
      "    accuracy                           0.82     21860\n",
      "   macro avg       0.81      0.82      0.81     21860\n",
      "weighted avg       0.83      0.82      0.82     21860\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "predict=classifier.predict(vectorizer.transform(fenci(x_test)))\n",
    "print(\"每个类别的精确率与召回率\\n\",classification_report(predict, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d0ff0f59-7c46-4520-855e-f8e425156ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open('test_5.txt','r',encoding='utf-8')\n",
    "\n",
    "L=[]\n",
    "m=f.readlines()\n",
    "for i in m:\n",
    "    n=i.strip('\\n')\n",
    "    x=n.split('\\t')\n",
    "    L.append(x[1])\n",
    "\n",
    "data_to_predict=pd.Series(L)\n",
    "# data_to_predict=np.matrix(data_to_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff4cb344-b128-4cf1-9509-f9651f947fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(603,)\n"
     ]
    }
   ],
   "source": [
    "print(data_to_predict.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "77a738a8-c314-4860-9fb0-374288c93ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict=classifier.predict(vectorizer.transform(fenci(data_to_predict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d23fd5f4-569c-41b2-b0ae-fe49558fb5c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 7 2 5 3 2 5 0 5 7 2 0 4 7 0 4 5 5 7 0 5 2 5 6 4 4 5 4 5 2 0 4 2 8 0 5 5\n",
      " 5 5 5 5 7 6 5 5 0 4 2 0 1 5 5 5 7 4 5 5 5 2 2 0 5 1 1 5 0 2 6 4 0 6 5 1 0\n",
      " 5 8 5 5 2 3 5 4 4 5 4 1 5 5 5 5 4 3 5 5 5 5 3 5 3 5 5 5 5 4 4 5 1 3 7 8 3\n",
      " 2 5 5 3 5 4 5 5 5 0 1 5 7 5 5 1 4 5 5 1 4 4 1 7 6 5 3 0 2 1 5 5 7 0 5 7 1\n",
      " 5 4 4 5 5 0 0 1 5 7 0 1 5 6 0 5 2 0 5 2 3 4 5 6 5 5 1 5 4 0 4 5 7 5 4 5 2\n",
      " 5 7 0 5 5 5 5 5 2 3 2 2 4 7 6 7 5 7 7 0 4 0 5 5 2 5 2 2 6 4 2 0 4 5 7 0 2\n",
      " 5 7 0 2 2 5 2 5 5 0 5 5 0 7 6 5 1 5 4 7 5 0 0 4 5 5 7 5 7 2 5 5 0 7 5 3 5\n",
      " 5 5 5 0 2 1 2 4 5 5 5 2 4 0 0 4 4 4 1 7 0 2 4 4 5 0 8 5 0 2 0 0 4 2 5 5 2\n",
      " 5 1 7 0 5 8 5 0 2 5 5 5 5 4 5 0 2 5 6 5 7 0 5 6 8 0 4 5 5 2 5 3 5 5 4 5 2\n",
      " 5 4 5 5 7 5 0 5 5 2 2 5 1 4 0 6 2 0 5 6 0 3 2 4 0 1 2 5 7 5 8 5 5 5 0 5 5\n",
      " 4 5 4 5 1 2 5 6 2 5 2 4 3 5 5 0 0 4 6 0 4 5 1 7 5 1 5 4 6 2 4 5 4 5 1 0 5\n",
      " 0 4 5 5 0 5 2 5 0 5 7 5 1 4 0 7 1 5 5 4 5 5 1 5 5 5 5 7 5 5 5 4 2 5 5 5 6\n",
      " 5 2 5 2 7 5 0 5 4 5 4 5 5 5 0 1 0 5 6 2 4 0 5 5 6 5 0 5 0 4 3 5 5 5 5 5 2\n",
      " 2 0 7 5 7 5 2 0 7 1 5 0 5 5 6 2 5 5 0 5 2 2 1 5 1 2 5 2 7 5 4 5 5 5 0 5 5\n",
      " 0 3 5 0 0 8 2 6 5 5 5 0 0 5 3 2 5 5 5 5 0 5 7 5 1 6 8 0 8 0 4 5 1 0 4 0 5\n",
      " 3 0 4 5 7 1 5 7 3 5 5 7 4 5 8 1 5 4 6 5 0 5 5 1 5 3 2 3 0 0 5 5 1 5 0 5 4\n",
      " 5 0 4 4 6 0 5 5 5 2 0]\n"
     ]
    }
   ],
   "source": [
    "print(predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257ff2c0-f563-4ffd-8cb7-92f3344b7358",
   "metadata": {},
   "source": [
    "## TF-IDF预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "76315037-6bca-4703-ad4a-c57dbe275364",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Software\\Anaconda\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['①①', '①②', '①③', '①④', '①⑤', '①⑥', '①⑦', '①⑧', '①⑨', '①Ａ', '①Ｂ', '①Ｃ', '①Ｄ', '①Ｅ', '①ａ', '①ｃ', '①ｄ', '①ｅ', '①ｆ', '①ｇ', '①ｈ', '①ｉ', '①ｏ', '②①', '②②', '②③', '②④', '②⑤', '②⑥', '②⑦', '②⑧', '②⑩', '②Ｂ', '②Ｇ', '②ａ', '②ｂ', '②ｄ', '②ｅ', '②ｆ', '②ｇ', '②ｈ', '②ｉ', '②ｊ', '③①', '③⑩', '③Ｆ', '③ａ', '③ｂ', '③ｃ', '③ｄ', '③ｅ', '③ｇ', '③ｈ', '④ａ', '④ｂ', '④ｃ', '④ｄ', '④ｅ', '⑤ａ', '⑤ｂ', '⑤ｄ', '⑤ｅ', '⑤ｆ', '１２'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8201738334858188"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#使用tf-idf把文本转为向量\n",
    "tv = TfidfVectorizer(stop_words=stopwords, max_features=5000, lowercase = False)\n",
    "tv.fit(x_train_fenci)\n",
    "#模型训练\n",
    "classifier.fit(tv.transform(fenci(x_train)), y_train)\n",
    "#利用训练好的模型测试\n",
    "classifier.score(tv.transform(fenci(x_test)), y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "174fd17d-da73-4d38-a1d0-c14fcdfcdd66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "每个类别的精确率与召回率\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.81      0.70      1044\n",
      "           1       0.74      0.81      0.77      2029\n",
      "           2       0.77      0.80      0.79      1531\n",
      "           3       0.81      0.80      0.80      2334\n",
      "           4       0.87      0.83      0.85      2855\n",
      "           5       0.89      0.86      0.88      3060\n",
      "           6       0.89      0.73      0.80      3999\n",
      "           7       0.89      0.89      0.89      3079\n",
      "           8       0.72      0.87      0.79      1929\n",
      "\n",
      "    accuracy                           0.82     21860\n",
      "   macro avg       0.80      0.82      0.81     21860\n",
      "weighted avg       0.83      0.82      0.82     21860\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "predict=classifier.predict(tv.transform(fenci(x_test)))\n",
    "print(\"每个类别的精确率与召回率\\n\",classification_report(predict, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fbae6c8-44ef-473c-8399-a85292516a2e",
   "metadata": {},
   "source": [
    "## N-gram 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1ae0e946-6426-4e82-b37d-2c17258821c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Software\\Anaconda\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['①①', '①②', '①③', '①④', '①⑤', '①⑥', '①⑦', '①⑧', '①⑨', '①Ａ', '①Ｂ', '①Ｃ', '①Ｄ', '①Ｅ', '①ａ', '①ｃ', '①ｄ', '①ｅ', '①ｆ', '①ｇ', '①ｈ', '①ｉ', '①ｏ', '②①', '②②', '②③', '②④', '②⑤', '②⑥', '②⑦', '②⑧', '②⑩', '②Ｂ', '②Ｇ', '②ａ', '②ｂ', '②ｄ', '②ｅ', '②ｆ', '②ｇ', '②ｈ', '②ｉ', '②ｊ', '③①', '③⑩', '③Ｆ', '③ａ', '③ｂ', '③ｃ', '③ｄ', '③ｅ', '③ｇ', '③ｈ', '④ａ', '④ｂ', '④ｃ', '④ｄ', '④ｅ', '⑤ａ', '⑤ｂ', '⑤ｄ', '⑤ｅ', '⑤ｆ', '１２'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8146386093321134"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tv_2gram = TfidfVectorizer(stop_words=stopwords, max_features=5000, ngram_range=(1,2),lowercase = False)\n",
    "tv_2gram.fit(x_train_fenci)\n",
    "#训练模型\n",
    "clf_2gram = MultinomialNB()\n",
    "clf_2gram.fit(tv_2gram.transform(fenci(x_train)), y_train)\n",
    "#预测\n",
    "clf_2gram.score(tv_2gram.transform(fenci(x_test)), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c28c78ba-57c2-4207-9d97-90f50e8bc0a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "每个类别的精确率与召回率\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.83      0.70      1009\n",
      "           1       0.73      0.81      0.77      1970\n",
      "           2       0.78      0.79      0.78      1556\n",
      "           3       0.80      0.79      0.80      2341\n",
      "           4       0.87      0.82      0.84      2875\n",
      "           5       0.89      0.85      0.87      3098\n",
      "           6       0.89      0.72      0.79      4067\n",
      "           7       0.88      0.90      0.89      3054\n",
      "           8       0.71      0.87      0.78      1890\n",
      "\n",
      "    accuracy                           0.81     21860\n",
      "   macro avg       0.79      0.82      0.80     21860\n",
      "weighted avg       0.82      0.81      0.82     21860\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "predict=clf_2gram.predict(tv_2gram.transform(fenci(x_test)))\n",
    "print(\"每个类别的精确率与召回率\\n\",classification_report(predict, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148d1be2-0413-427a-a005-16ec95209401",
   "metadata": {},
   "source": [
    "## 3-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fd6dc1bd-7b71-4d90-baf5-159284dd54c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Software\\Anaconda\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['①①', '①②', '①③', '①④', '①⑤', '①⑥', '①⑦', '①⑧', '①⑨', '①Ａ', '①Ｂ', '①Ｃ', '①Ｄ', '①Ｅ', '①ａ', '①ｃ', '①ｄ', '①ｅ', '①ｆ', '①ｇ', '①ｈ', '①ｉ', '①ｏ', '②①', '②②', '②③', '②④', '②⑤', '②⑥', '②⑦', '②⑧', '②⑩', '②Ｂ', '②Ｇ', '②ａ', '②ｂ', '②ｄ', '②ｅ', '②ｆ', '②ｇ', '②ｈ', '②ｉ', '②ｊ', '③①', '③⑩', '③Ｆ', '③ａ', '③ｂ', '③ｃ', '③ｄ', '③ｅ', '③ｇ', '③ｈ', '④ａ', '④ｂ', '④ｃ', '④ｄ', '④ｅ', '⑤ａ', '⑤ｂ', '⑤ｄ', '⑤ｅ', '⑤ｆ', '１２'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.3442622950819672"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tv_3gram = TfidfVectorizer(stop_words=stopwords, max_features=5000, ngram_range=(1,3),lowercase = False)\n",
    "tv_3gram.fit(x_train_fenci)\n",
    "#训练模型\n",
    "clf_3gram = MultinomialNB()\n",
    "clf_3gram.fit(tv_3gram.transform(fenci(x_train)), y_train)\n",
    "#预测\n",
    "clf_3gram.score(tv_3gram.transform(fenci(x_test)), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a677781f-da97-4706-9e25-d4aa66a88f24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "每个类别的精确率与召回率\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.40      0.36         5\n",
      "           1       0.55      0.75      0.63         8\n",
      "           2       0.40      1.00      0.57         2\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.57      0.80      0.67         5\n",
      "           5       1.00      0.06      0.12        31\n",
      "           6       0.00      0.00      0.00         0\n",
      "           7       0.50      0.38      0.43         8\n",
      "           8       0.22      1.00      0.36         2\n",
      "\n",
      "    accuracy                           0.34        61\n",
      "   macro avg       0.40      0.49      0.35        61\n",
      "weighted avg       0.74      0.34      0.32        61\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Software\\Anaconda\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "D:\\Software\\Anaconda\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "D:\\Software\\Anaconda\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "predict=clf_3gram.predict(tv_3gram.transform(fenci(x_test)))\n",
    "print(\"每个类别的精确率与召回率\\n\",classification_report(predict, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493c8360-1db2-4f32-8f49-f7f5d906cb40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
