{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c74a9ba5-596a-461e-a655-82bfafeacf1e",
   "metadata": {},
   "source": [
    "## 朴素贝叶斯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "92a151b4-f1df-443f-b091-61ffa763fa5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open('Data/2/train.txt','r',encoding='utf-8')\n",
    "f=open('Data/2/3.csv','a',newline='',encoding='utf-8')\n",
    "\n",
    "csv_writer = csv.writer(f)\n",
    "csv_writer.writerow([\"title\",\"tag\"])\n",
    "m=data.readlines()\n",
    "\n",
    "\n",
    "for i in m:\n",
    "    n=i.strip('\\n')\n",
    "    L=n.split('\\t')\n",
    "    # print(L)\n",
    "    csv_writer.writerow(L)\n",
    "    \n",
    "f.close()\n",
    "data.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "8100e309-1117-40f5-99aa-2563d33bcc07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据数量： 5801\n"
     ]
    }
   ],
   "source": [
    "# data=pd.read_csv('Data/data_new/3.csv')\n",
    "data = pd.read_csv('Data/2/3.csv', error_bad_lines=False)\n",
    "\n",
    "data.head()\n",
    "data_length = len(data)\n",
    "print(\"数据数量：\", data_length)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "58655cee-9191-4c17-a8d4-766b9b6b47c5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 划分训练集测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "cd55aea9-ce63-45d4-a8a9-a1ebeda2bbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(data['title'], data['tag'], random_state=1)\n",
    "\n",
    "# x_train, x_test, y_train, y_test = train_test_split(data['label'], data['text_a'], random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "7525546c-5a80-4a08-92f4-a56dd1dc7858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1451\n",
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "print(x_test.shape[0])\n",
    "print(type(y_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7c776a62-45af-4a9a-acf2-21d67b2b4ae4",
   "metadata": {},
   "source": [
    "## 分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "0a189285-b270-42a2-8541-412c74ceff3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "804                   北京 联众 涉赌 36 人 被 抓   港股 联众 上午 已经 停牌\n",
       "1030                   历时 9 月   利尔 化学 8.5 亿 可转债 募资 获 新进展\n",
       "587                     2 名 男子 因 酒后 随地 小便 与 他人 起 争执 被 砍死\n",
       "4497                               台湾 在 马来西亚 被 掳 女 游客 获释\n",
       "2076    “ 尔康 和 小燕子 ” 居然 有 了 小孩 ！ 两人 的 爱情 简直 就是 部虐 狗 大戏 啊\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fenci(train_data):\n",
    "    words_df = train_data.apply(lambda x:' '.join(jieba.cut(x)))\n",
    "    return words_df\n",
    "\n",
    "x_train_fenci = fenci(x_train)\n",
    "x_train_fenci[:5]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "59eaae1b-0766-4fcc-b5b2-9f5fe7e43cdc",
   "metadata": {},
   "source": [
    "## 停词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "6afe0bb5-b981-4208-b3f0-3ce1179496ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = open(\"hit_stopwords.txt\",encoding='utf-8')\n",
    "stopwords_lst = infile.readlines()\n",
    "stopwords = [x.strip() for x in stopwords_lst]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "beedbb5e-bf2c-415c-8a19-b5d9d84472d0",
   "metadata": {},
   "source": [
    "## 向量化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b7dfd0cc-e8c1-4a6b-9166-fcc0afdaa1ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhanghongwei/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['lex', '①①', '①②', '①③', '①④', '①⑤', '①⑥', '①⑦', '①⑧', '①⑨', '①ａ', '①ｂ', '①ｃ', '①ｄ', '①ｅ', '①ｆ', '①ｇ', '①ｈ', '①ｉ', '①ｏ', '②①', '②②', '②③', '②④', '②⑤', '②⑥', '②⑦', '②⑧', '②⑩', '②ａ', '②ｂ', '②ｄ', '②ｅ', '②ｆ', '②ｇ', '②ｈ', '②ｉ', '②ｊ', '③①', '③⑩', '③ａ', '③ｂ', '③ｃ', '③ｄ', '③ｅ', '③ｆ', '③ｇ', '③ｈ', '④ａ', '④ｂ', '④ｃ', '④ｄ', '④ｅ', '⑤ａ', '⑤ｂ', '⑤ｄ', '⑤ｅ', '⑤ｆ', '１２', 'ｌｉ', 'ｚｘｆｉｔｌ'] not in stop_words.\n",
      "  % sorted(inconsistent)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CountVectorizer(max_features=5000,\n",
       "                stop_words=['———', '》），', '）÷（１－', '”，', '）、', '＝（', ':', '→',\n",
       "                            '℃', '&', '*', '一一', '~~~~', '’', '.', '『', '.一',\n",
       "                            './', '--', '』', '＝″', '【', '［＊］', '｝＞', '［⑤］］',\n",
       "                            '［①Ｄ］', 'ｃ］', 'ｎｇ昉', '＊', '//', ...])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words=stopwords, max_features=5000)\n",
    "vectorizer.fit(x_train_fenci)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "6a1759af-ddcc-4572-9637-4348f2fb1aa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8607856650585803"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "classifier = MultinomialNB()\n",
    "#模型训练\n",
    "classifier.fit(vectorizer.transform(x_train_fenci), y_train)\n",
    "#使用训练好的模型进行预测\n",
    "classifier.score(vectorizer.transform(fenci(x_test)), y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c5c3def4-b30f-45bc-92b3-a3ede9750d93",
   "metadata": {},
   "source": [
    "## 预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "5fd54599-f752-4f5d-b563-1f9506e66c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "每个类别的精确率与召回率               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.89      0.86       103\n",
      "           1       0.74      0.87      0.80        85\n",
      "           2       0.72      0.84      0.78       110\n",
      "           3       0.90      0.82      0.86       386\n",
      "           4       0.96      0.88      0.92       241\n",
      "           5       0.86      0.87      0.86       330\n",
      "           6       0.90      0.90      0.90       193\n",
      "           7       0.23      1.00      0.38         3\n",
      "         tag       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.86      1451\n",
      "   macro avg       0.68      0.79      0.71      1451\n",
      "weighted avg       0.87      0.86      0.86      1451\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhanghongwei/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/zhanghongwei/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/zhanghongwei/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "predict=classifier.predict(vectorizer.transform(fenci(x_test)))\n",
    "print(\"每个类别的精确率与召回率\",classification_report(predict, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d0ff0f59-7c46-4520-855e-f8e425156ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open('test_7.txt','r',encoding='utf-8')\n",
    "\n",
    "L=[]\n",
    "m=f.readlines()\n",
    "for i in m:\n",
    "    n=i.strip('\\n')\n",
    "    x=n.split('\\t')\n",
    "    L.append(x[1])\n",
    "\n",
    "data_to_predict=pd.Series(L)\n",
    "# data_to_predict=np.matrix(data_to_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "ff4cb344-b128-4cf1-9509-f9651f947fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(600,)\n"
     ]
    }
   ],
   "source": [
    "print(data_to_predict.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "77a738a8-c314-4860-9fb0-374288c93ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict=classifier.predict(vectorizer.transform(fenci(data_to_predict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "d23fd5f4-569c-41b2-b0ae-fe49558fb5c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['4' '1' '5' '6' '5' '3' '6' '3' '3' '4' '3' '3' '0' '3' '3' '0' '0' '6'\n",
      " '2' '6' '5' '3' '3' '3' '4' '4' '1' '3' '5' '4' '3' '3' '0' '4' '4' '5'\n",
      " '3' '6' '6' '3' '6' '6' '1' '3' '3' '5' '1' '4' '1' '3' '0' '3' '0' '4'\n",
      " '3' '3' '4' '4' '4' '3' '4' '1' '3' '4' '4' '4' '3' '3' '3' '3' '4' '3'\n",
      " '3' '3' '2' '6' '0' '3' '5' '3' '4' '5' '4' '0' '0' '5' '5' '4' '3' '1'\n",
      " '3' '4' '1' '4' '5' '1' '4' '3' '2' '5' '5' '5' '2' '6' '3' '6' '3' '3'\n",
      " '1' '4' '1' '4' '4' '4' '0' '3' '2' '6' '5' '3' '5' '0' '3' '0' '5' '4'\n",
      " '3' '2' '2' '0' '6' '3' '2' '3' '4' '2' '4' '6' '4' '5' '2' '3' '5' '3'\n",
      " '6' '3' '6' '4' '4' '3' '3' '5' '6' '4' '5' '0' '3' '3' '4' '3' '4' '4'\n",
      " '3' '5' '1' '5' '5' '4' '2' '3' '0' '3' '4' '3' '5' '3' '6' '0' '6' '4'\n",
      " '1' '3' '2' '4' '5' '0' '4' '3' '1' '6' '3' '3' '0' '0' '3' '3' '3' '3'\n",
      " '3' '5' '3' '6' '6' '2' '3' '6' '1' '1' '1' '1' '5' '3' '1' '3' '4' '6'\n",
      " '1' '3' '3' '3' '3' '6' '3' '1' '3' '4' '4' '6' '5' '0' '4' '1' '6' '4'\n",
      " '5' '3' '0' '3' '3' '1' '0' '0' '5' '1' '6' '4' '2' '3' '6' '3' '6' '3'\n",
      " '5' '2' '4' '6' '4' '6' '4' '3' '6' '4' '5' '5' '3' '6' '5' '4' '3' '1'\n",
      " '4' '3' '5' '3' '3' '5' '0' '6' '6' '5' '6' '4' '6' '4' '5' '5' '6' '6'\n",
      " '2' '6' '5' '2' '6' '4' '0' '3' '4' '4' '2' '2' '2' '4' '1' '0' '3' '6'\n",
      " '2' '2' '5' '3' '5' '1' '3' '4' '4' '3' '3' '4' '3' '2' '5' '6' '3' '5'\n",
      " '6' '3' '3' '3' '4' '5' '3' '3' '3' '4' '2' '5' '5' '0' '1' '5' '5' '1'\n",
      " '5' '3' '6' '3' '4' '2' '3' '4' '3' '3' '1' '5' '6' '3' '4' '0' '3' '3'\n",
      " '6' '4' '5' '4' '3' '0' '1' '2' '3' '0' '6' '0' '4' '0' '3' '5' '4' '0'\n",
      " '2' '3' '6' '4' '5' '0' '4' '2' '1' '3' '6' '5' '4' '6' '3' '1' '6' '3'\n",
      " '6' '4' '5' '0' '6' '3' '3' '3' '1' '4' '3' '5' '6' '2' '3' '6' '0' '2'\n",
      " '3' '1' '2' '2' '3' '4' '4' '6' '5' '6' '4' '5' '4' '3' '6' '1' '4' '3'\n",
      " '3' '6' '1' '3' '6' '4' '1' '2' '5' '3' '3' '2' '3' '6' '2' '0' '4' '4'\n",
      " '3' '3' '0' '3' '3' '1' '4' '4' '6' '5' '1' '3' '6' '6' '0' '5' '3' '3'\n",
      " '5' '4' '1' '0' '4' '5' '4' '6' '5' '4' '4' '3' '3' '4' '0' '3' '2' '2'\n",
      " '3' '3' '4' '4' '2' '4' '2' '4' '3' '0' '4' '3' '5' '5' '4' '6' '2' '1'\n",
      " '0' '5' '6' '4' '1' '3' '4' '0' '3' '5' '6' '3' '0' '4' '6' '4' '1' '0'\n",
      " '3' '6' '1' '2' '5' '3' '2' '4' '6' '3' '0' '0' '6' '3' '4' '3' '5' '4'\n",
      " '4' '4' '3' '3' '6' '0' '1' '5' '5' '3' '3' '5' '5' '5' '3' '5' '3' '5'\n",
      " '3' '6' '4' '6' '0' '3' '6' '3' '3' '4' '3' '3' '0' '0' '4' '3' '4' '0'\n",
      " '3' '2' '0' '3' '3' '6' '0' '5' '1' '6' '1' '3' '5' '3' '3' '2' '0' '0'\n",
      " '3' '6' '6' '4' '3' '0']\n"
     ]
    }
   ],
   "source": [
    "print(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "\n",
    "def fenci(news_title):\n",
    "    words = ' '.join(jieba.cut(news_title))\n",
    "    return words\n",
    "\n",
    "f_r = open(\"test_6.txt\", \"r\", encoding=\"utf-8\")\n",
    "f_w = open(\"result_6_zhw.txt\", \"w\", encoding=\"utf-8\")\n",
    "\n",
    "m = f_r.readlines()\n",
    "for i in m:\n",
    "    n = i.strip('\\n')\n",
    "    L = n.split('\\t')\n",
    "    data_id = L[0]\n",
    "    news_title = L[1]\n",
    "    prediction = classifier.predict(vectorizer.transform([fenci(news_title)]))\n",
    "    line = data_id + '\\t' + str(prediction[0]) + '\\n'\n",
    "    f_w.write(line)\n",
    "\n",
    "f_r.close()\n",
    "f_w.close()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "257ff2c0-f563-4ffd-8cb7-92f3344b7358",
   "metadata": {},
   "source": [
    "## TF-IDF预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "76315037-6bca-4703-ad4a-c57dbe275364",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhanghongwei/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['①①', '①②', '①③', '①④', '①⑤', '①⑥', '①⑦', '①⑧', '①⑨', '①Ａ', '①Ｂ', '①Ｃ', '①Ｄ', '①Ｅ', '①ａ', '①ｃ', '①ｄ', '①ｅ', '①ｆ', '①ｇ', '①ｈ', '①ｉ', '①ｏ', '②①', '②②', '②③', '②④', '②⑤', '②⑥', '②⑦', '②⑧', '②⑩', '②Ｂ', '②Ｇ', '②ａ', '②ｂ', '②ｄ', '②ｅ', '②ｆ', '②ｇ', '②ｈ', '②ｉ', '②ｊ', '③①', '③⑩', '③Ｆ', '③ａ', '③ｂ', '③ｃ', '③ｄ', '③ｅ', '③ｇ', '③ｈ', '④ａ', '④ｂ', '④ｃ', '④ｄ', '④ｅ', '⑤ａ', '⑤ｂ', '⑤ｄ', '⑤ｅ', '⑤ｆ', '１２'] not in stop_words.\n",
      "  % sorted(inconsistent)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Series' object has no attribute 'decode'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-98-c6dd38da2998>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_fenci\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#模型训练\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfenci\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;31m#利用训练好的模型测试\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# predict=classifier.predict(tv.transform(fenci(x_test)))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-97-fff604745949>\u001b[0m in \u001b[0;36mfenci\u001b[0;34m(news_title)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfenci\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnews_title\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjieba\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcut\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnews_title\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/jieba/__init__.py\u001b[0m in \u001b[0;36mcut\u001b[0;34m(self, sentence, cut_all, HMM, use_paddle)\u001b[0m\n\u001b[1;32m    298\u001b[0m         \"\"\"\n\u001b[1;32m    299\u001b[0m         \u001b[0mis_paddle_installed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_paddle_install\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'is_paddle_installed'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m         \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstrdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0muse_paddle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_paddle_installed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m             \u001b[0;31m# if sentence is null, it will raise core exception in paddle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/jieba/_compat.py\u001b[0m in \u001b[0;36mstrdecode\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m             \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mUnicodeDecodeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'gbk'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   4374\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4375\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4376\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4378\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Series' object has no attribute 'decode'"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#使用tf-idf把文本转为向量\n",
    "tv = TfidfVectorizer(stop_words=stopwords, max_features=5000, lowercase = False)\n",
    "tv.fit(x_train_fenci)\n",
    "#模型训练\n",
    "classifier.fit(tv.transform(fenci(x_train)), y_train)\n",
    "#利用训练好的模型测试\n",
    "# predict=classifier.predict(tv.transform(fenci(x_test)))\n",
    "classifier.score(tv.transform(fenci(x_test)), y_test)\n",
    "\n",
    "# print(\"每个类别的精确率与召回率\",classification_report(predict, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17415b82-3489-4050-9bca-91e142818c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict=classifier.predict(tv.transform(fenci(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ef60f2-6ace-4be6-a714-19f56447f098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "每个类别的精确率与召回率               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.78      0.69      2322\n",
      "           1       0.61      0.84      0.71      2106\n",
      "           2       0.80      0.77      0.79      3256\n",
      "           3       0.91      0.79      0.85     10586\n",
      "           4       0.90      0.85      0.87      5816\n",
      "           5       0.85      0.82      0.84      9142\n",
      "           6       0.85      0.93      0.89      4876\n",
      "           7       0.02      0.50      0.03        10\n",
      "\n",
      "    accuracy                           0.83     38114\n",
      "   macro avg       0.69      0.79      0.71     38114\n",
      "weighted avg       0.84      0.83      0.83     38114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"每个类别的精确率与召回率\",classification_report(predict, y_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0fbae6c8-44ef-473c-8399-a85292516a2e",
   "metadata": {},
   "source": [
    "## N-gram 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae0e946-6426-4e82-b37d-2c17258821c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhanghongwei/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['①①', '①②', '①③', '①④', '①⑤', '①⑥', '①⑦', '①⑧', '①⑨', '①Ａ', '①Ｂ', '①Ｃ', '①Ｄ', '①Ｅ', '①ａ', '①ｃ', '①ｄ', '①ｅ', '①ｆ', '①ｇ', '①ｈ', '①ｉ', '①ｏ', '②①', '②②', '②③', '②④', '②⑤', '②⑥', '②⑦', '②⑧', '②⑩', '②Ｂ', '②Ｇ', '②ａ', '②ｂ', '②ｄ', '②ｅ', '②ｆ', '②ｇ', '②ｈ', '②ｉ', '②ｊ', '③①', '③⑩', '③Ｆ', '③ａ', '③ｂ', '③ｃ', '③ｄ', '③ｅ', '③ｇ', '③ｈ', '④ａ', '④ｂ', '④ｃ', '④ｄ', '④ｅ', '⑤ａ', '⑤ｂ', '⑤ｄ', '⑤ｅ', '⑤ｆ', '１２'] not in stop_words.\n",
      "  % sorted(inconsistent)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8230046701999265"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tv_2gram = TfidfVectorizer(stop_words=stopwords, max_features=5000, ngram_range=(1,2),lowercase = False)\n",
    "tv_2gram.fit(x_train_fenci)\n",
    "#训练模型\n",
    "clf_2gram = MultinomialNB()\n",
    "clf_2gram.fit(tv_2gram.transform(fenci(x_train)), y_train)\n",
    "#预测\n",
    "clf_2gram.score(tv_2gram.transform(fenci(x_test)), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6752898f-9246-4d6c-b50f-bf5131bc3911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "每个类别的精确率与召回率               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.79      0.68      2224\n",
      "           1       0.61      0.84      0.70      2111\n",
      "           2       0.81      0.77      0.79      3297\n",
      "           3       0.91      0.78      0.84     10680\n",
      "           4       0.89      0.85      0.87      5800\n",
      "           5       0.85      0.82      0.84      9104\n",
      "           6       0.85      0.92      0.89      4889\n",
      "           7       0.02      0.56      0.03         9\n",
      "\n",
      "    accuracy                           0.82     38114\n",
      "   macro avg       0.69      0.79      0.71     38114\n",
      "weighted avg       0.84      0.82      0.83     38114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predict=clf_2gram.predict(tv_2gram.transform(fenci(x_test)))\n",
    "print(\"每个类别的精确率与召回率\",classification_report(predict, y_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4de2ae70-234a-4128-aad6-8d423b609294",
   "metadata": {},
   "source": [
    "## 3-gram模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28c78ba-57c2-4207-9d97-90f50e8bc0a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhanghongwei/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['①①', '①②', '①③', '①④', '①⑤', '①⑥', '①⑦', '①⑧', '①⑨', '①Ａ', '①Ｂ', '①Ｃ', '①Ｄ', '①Ｅ', '①ａ', '①ｃ', '①ｄ', '①ｅ', '①ｆ', '①ｇ', '①ｈ', '①ｉ', '①ｏ', '②①', '②②', '②③', '②④', '②⑤', '②⑥', '②⑦', '②⑧', '②⑩', '②Ｂ', '②Ｇ', '②ａ', '②ｂ', '②ｄ', '②ｅ', '②ｆ', '②ｇ', '②ｈ', '②ｉ', '②ｊ', '③①', '③⑩', '③Ｆ', '③ａ', '③ｂ', '③ｃ', '③ｄ', '③ｅ', '③ｇ', '③ｈ', '④ａ', '④ｂ', '④ｃ', '④ｄ', '④ｅ', '⑤ａ', '⑤ｂ', '⑤ｄ', '⑤ｅ', '⑤ｆ', '１２'] not in stop_words.\n",
      "  % sorted(inconsistent)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.44167497507477566"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tv_3gram = TfidfVectorizer(stop_words=stopwords, max_features=5000, ngram_range=(2,3),lowercase = False)\n",
    "tv_3gram.fit(x_train_fenci)\n",
    "#训练模型\n",
    "clf_3gram = MultinomialNB()\n",
    "clf_3gram.fit(tv_3gram.transform(fenci(x_train)), y_train)\n",
    "#预测\n",
    "clf_3gram.score(tv_3gram.transform(fenci(x_test)), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b8f828-18a7-476e-8757-01207efd46ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "每个类别的精确率与召回率               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.29      0.65      0.40      2543\n",
      "           1       0.33      0.72      0.45      1985\n",
      "           2       0.89      0.33      0.48     17272\n",
      "           3       0.33      0.60      0.43      2340\n",
      "           4       0.20      0.85      0.32       307\n",
      "           5       0.06      0.66      0.11       125\n",
      "           6       0.11      0.88      0.19       112\n",
      "           7       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.43     24684\n",
      "   macro avg       0.28      0.59      0.30     24684\n",
      "weighted avg       0.71      0.43      0.46     24684\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhanghongwei/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/zhanghongwei/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/zhanghongwei/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "predict=clf_3gram.predict(tv_3gram.transform(fenci(x_test)))\n",
    "print(\"每个类别的精确率与召回率\",classification_report(predict, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43eea18-4e41-4245-8e27-cd1586f3763b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
