{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c74a9ba5-596a-461e-a655-82bfafeacf1e",
   "metadata": {},
   "source": [
    "## 5%数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35d365a0-91c5-4886-9d6d-242119bae08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import jieba\n",
    "import csv\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e738c97-072c-4915-ac5f-c69612c8bd04",
   "metadata": {},
   "source": [
    "## 载入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92a151b4-f1df-443f-b091-61ffa763fa5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open('Data/5%/test.txt','r',encoding='utf-8')\n",
    "f=open('Data/5%/5%.csv','a',newline='',encoding='utf-8')\n",
    "\n",
    "csv_writer = csv.writer(f)\n",
    "\n",
    "m=data.readlines()\n",
    "\n",
    "csv_writer.writerow([\"title\",\"tag\"])\n",
    "\n",
    "for i in m:\n",
    "    n=i.strip('\\n')\n",
    "    L=n.split('\\t')\n",
    "    # print(L)\n",
    "    csv_writer.writerow(L)\n",
    "    \n",
    "f.close()\n",
    "data.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8100e309-1117-40f5-99aa-2563d33bcc07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>今年一季度全国纪检监察机关处分省部级干部16人</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>习近平：携手迎接挑战 合作开创未来</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>瞭望丨引领人类海洋文明发展方向</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>广东省老区湾区同携手 接续奋进新时代</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>关注全民健身：健身场地少、消费高等难题怎么破？</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     title  tag\n",
       "0  今年一季度全国纪检监察机关处分省部级干部16人    0\n",
       "1       习近平：携手迎接挑战 合作开创未来     0\n",
       "2          瞭望丨引领人类海洋文明发展方向    0\n",
       "3       广东省老区湾区同携手 接续奋进新时代    0\n",
       "4  关注全民健身：健身场地少、消费高等难题怎么破？    0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.read_csv('Data/5%/5%.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58655cee-9191-4c17-a8d4-766b9b6b47c5",
   "metadata": {},
   "source": [
    "## 划分训练集测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd55aea9-ce63-45d4-a8a9-a1ebeda2bbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(data['title'], data['tag'], random_state=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7525546c-5a80-4a08-92f4-a56dd1dc7858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180\n",
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "print(x_test.shape[0])\n",
    "print(type(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c776a62-45af-4a9a-acf2-21d67b2b4ae4",
   "metadata": {},
   "source": [
    "## 分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0a189285-b270-42a2-8541-412c74ceff3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\32852\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.697 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "262                 妖股 第二次 砸 盘 ， 欢乐 海岸 还会来 反包 吗 ？\n",
       "388    国家 奥运 精英 运动员 代表团 将 于 12 月 19 日至 21 日 访问 澳门\n",
       "352              美 发布 所谓 涉港 报告 妄评 我治港 政策   外交部 回应\n",
       "366          中方 ： 奉劝 英国 有关 政客 不要 在 台湾 问题 上 “ 玩火 ”\n",
       "259                 商标 转让 和 商标 变更 一样 吗 ？ 不 一样 吗 ？\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fenci(train_data):\n",
    "    words_df = train_data.apply(lambda x:' '.join(jieba.cut(x)))\n",
    "    return words_df\n",
    "\n",
    "x_train_fenci = fenci(x_train)\n",
    "x_train_fenci[:5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59eaae1b-0766-4fcc-b5b2-9f5fe7e43cdc",
   "metadata": {},
   "source": [
    "## 停词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6afe0bb5-b981-4208-b3f0-3ce1179496ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = open(\"hit_stopwords.txt\",encoding='utf-8')\n",
    "stopwords_lst = infile.readlines()\n",
    "stopwords = [x.strip() for x in stopwords_lst]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beedbb5e-bf2c-415c-8a19-b5d9d84472d0",
   "metadata": {},
   "source": [
    "## 向量化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b7dfd0cc-e8c1-4a6b-9166-fcc0afdaa1ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Software\\Anaconda\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['lex', '①①', '①②', '①③', '①④', '①⑤', '①⑥', '①⑦', '①⑧', '①⑨', '①ａ', '①ｂ', '①ｃ', '①ｄ', '①ｅ', '①ｆ', '①ｇ', '①ｈ', '①ｉ', '①ｏ', '②①', '②②', '②③', '②④', '②⑤', '②⑥', '②⑦', '②⑧', '②⑩', '②ａ', '②ｂ', '②ｄ', '②ｅ', '②ｆ', '②ｇ', '②ｈ', '②ｉ', '②ｊ', '③①', '③⑩', '③ａ', '③ｂ', '③ｃ', '③ｄ', '③ｅ', '③ｆ', '③ｇ', '③ｈ', '④ａ', '④ｂ', '④ｃ', '④ｄ', '④ｅ', '⑤ａ', '⑤ｂ', '⑤ｄ', '⑤ｅ', '⑤ｆ', '１２', 'ｌｉ', 'ｚｘｆｉｔｌ'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CountVectorizer(max_features=5000,\n",
       "                stop_words=['———', '》），', '）÷（１－', '”，', '）、', '＝（', ':', '→',\n",
       "                            '℃', '&', '*', '一一', '~~~~', '’', '.', '『', '.一',\n",
       "                            './', '--', '』', '＝″', '【', '［＊］', '｝＞', '［⑤］］',\n",
       "                            '［①Ｄ］', 'ｃ］', 'ｎｇ昉', '＊', '//', ...])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words=stopwords, max_features=5000)\n",
    "vectorizer.fit(x_train_fenci)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6a1759af-ddcc-4572-9637-4348f2fb1aa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5833333333333334"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "classifier = MultinomialNB()\n",
    "#模型训练\n",
    "classifier.fit(vectorizer.transform(x_train_fenci), y_train)\n",
    "#使用训练好的模型进行预测\n",
    "classifier.score(vectorizer.transform(fenci(x_test)), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c3def4-b30f-45bc-92b3-a3ede9750d93",
   "metadata": {},
   "source": [
    "## 预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5fd54599-f752-4f5d-b563-1f9506e66c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "每个类别的精确率与召回率\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.80      0.56        15\n",
      "           1       0.70      0.59      0.64        27\n",
      "           2       0.86      0.33      0.48        36\n",
      "           3       0.50      0.37      0.42        19\n",
      "           4       0.72      0.54      0.62        24\n",
      "           5       0.57      0.92      0.71        13\n",
      "           6       0.45      0.82      0.58        11\n",
      "           7       0.67      0.64      0.65        22\n",
      "           8       0.48      0.77      0.59        13\n",
      "\n",
      "    accuracy                           0.58       180\n",
      "   macro avg       0.60      0.64      0.58       180\n",
      "weighted avg       0.65      0.58      0.57       180\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "predict=classifier.predict(vectorizer.transform(fenci(x_test)))\n",
    "print(\"每个类别的精确率与召回率\\n\",classification_report(predict, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d0ff0f59-7c46-4520-855e-f8e425156ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open('test_5.txt','r',encoding='utf-8')\n",
    "\n",
    "L=[]\n",
    "m=f.readlines()\n",
    "for i in m:\n",
    "    n=i.strip('\\n')\n",
    "    x=n.split('\\t')\n",
    "    L.append(x[1])\n",
    "\n",
    "data_to_predict=pd.Series(L)\n",
    "# data_to_predict=np.matrix(data_to_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ff4cb344-b128-4cf1-9509-f9651f947fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(603,)\n"
     ]
    }
   ],
   "source": [
    "print(data_to_predict.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "77a738a8-c314-4860-9fb0-374288c93ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict=classifier.predict(vectorizer.transform(fenci(data_to_predict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d23fd5f4-569c-41b2-b0ae-fe49558fb5c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 7 4 6 3 1 2 1 1 2 7 4 4 7 3 4 1 2 1 5 3 4 2 7 4 3 2 1 3 4 4 4 1 8 0 5 2\n",
      " 2 2 3 5 7 6 6 5 7 4 3 3 1 5 5 5 7 3 5 6 5 2 4 3 0 1 1 2 0 3 7 2 0 6 0 1 0\n",
      " 2 8 2 5 5 3 4 2 6 2 4 3 2 3 3 6 2 3 3 2 4 0 3 2 3 5 5 5 7 4 4 3 1 2 7 4 2\n",
      " 0 2 3 3 5 4 8 8 7 0 1 5 0 0 2 2 4 3 1 1 4 4 4 1 6 7 4 0 2 7 3 2 4 5 2 4 1\n",
      " 0 3 4 3 5 3 0 1 8 7 8 1 2 6 3 5 2 3 1 2 3 1 6 7 6 3 1 2 0 3 2 2 7 6 4 3 1\n",
      " 2 2 6 5 2 5 8 2 3 2 2 2 4 1 1 7 2 1 4 1 4 1 5 2 2 4 0 2 6 4 2 4 4 3 8 1 2\n",
      " 5 3 3 2 2 4 3 2 5 4 3 2 0 7 6 2 3 2 4 3 2 2 4 4 5 4 7 5 4 6 2 3 7 6 5 3 2\n",
      " 2 3 8 4 4 1 2 3 7 0 2 4 6 1 3 4 2 4 6 7 7 2 1 4 2 0 7 6 3 2 4 0 4 6 2 0 0\n",
      " 2 8 5 1 6 8 3 2 8 2 0 8 8 0 7 1 2 5 4 2 1 0 6 6 6 8 4 3 3 2 5 0 5 6 4 4 2\n",
      " 8 3 3 0 7 0 0 2 2 2 2 6 1 4 7 6 2 4 3 1 4 6 4 4 4 1 2 0 1 1 8 0 3 2 1 5 1\n",
      " 4 0 6 5 1 2 5 1 6 5 2 4 3 5 2 0 0 4 6 4 4 0 1 1 4 4 6 4 6 2 3 3 4 2 1 3 5\n",
      " 0 4 3 5 0 5 2 5 4 5 4 4 3 7 0 7 1 0 7 4 3 3 1 5 6 8 5 3 2 8 5 6 4 2 8 2 6\n",
      " 2 2 2 0 4 7 0 5 4 3 4 5 3 2 0 1 0 0 6 2 4 0 4 4 7 2 1 0 0 3 1 6 4 1 8 7 2\n",
      " 2 1 0 5 1 5 2 0 5 1 2 0 2 6 6 2 3 5 4 3 2 4 1 8 1 1 2 2 6 3 6 1 2 8 0 2 5\n",
      " 0 3 6 3 0 4 2 6 2 6 3 0 0 5 4 2 2 2 6 0 5 3 7 3 0 4 8 0 2 1 4 4 1 3 4 0 5\n",
      " 3 0 2 2 7 4 8 7 3 3 3 8 4 3 4 1 3 4 6 5 0 2 7 1 3 3 5 2 1 0 2 6 1 6 0 3 4\n",
      " 2 7 3 4 8 0 2 3 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "print(predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257ff2c0-f563-4ffd-8cb7-92f3344b7358",
   "metadata": {},
   "source": [
    "## TF-IDF预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "76315037-6bca-4703-ad4a-c57dbe275364",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Software\\Anaconda\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['①①', '①②', '①③', '①④', '①⑤', '①⑥', '①⑦', '①⑧', '①⑨', '①Ａ', '①Ｂ', '①Ｃ', '①Ｄ', '①Ｅ', '①ａ', '①ｃ', '①ｄ', '①ｅ', '①ｆ', '①ｇ', '①ｈ', '①ｉ', '①ｏ', '②①', '②②', '②③', '②④', '②⑤', '②⑥', '②⑦', '②⑧', '②⑩', '②Ｂ', '②Ｇ', '②ａ', '②ｂ', '②ｄ', '②ｅ', '②ｆ', '②ｇ', '②ｈ', '②ｉ', '②ｊ', '③①', '③⑩', '③Ｆ', '③ａ', '③ｂ', '③ｃ', '③ｄ', '③ｅ', '③ｇ', '③ｈ', '④ａ', '④ｂ', '④ｃ', '④ｄ', '④ｅ', '⑤ａ', '⑤ｂ', '⑤ｄ', '⑤ｅ', '⑤ｆ', '１２'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6055555555555555"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#使用tf-idf把文本转为向量\n",
    "tv = TfidfVectorizer(stop_words=stopwords, max_features=5000, lowercase = False)\n",
    "tv.fit(x_train_fenci)\n",
    "#模型训练\n",
    "classifier.fit(tv.transform(fenci(x_train)), y_train)\n",
    "#利用训练好的模型测试\n",
    "classifier.score(tv.transform(fenci(x_test)), y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "174fd17d-da73-4d38-a1d0-c14fcdfcdd66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "每个类别的精确率与召回率\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.80      0.56        15\n",
      "           1       0.65      0.75      0.70        20\n",
      "           2       0.86      0.32      0.46        38\n",
      "           3       0.64      0.38      0.47        24\n",
      "           4       0.72      0.54      0.62        24\n",
      "           5       0.52      0.92      0.67        12\n",
      "           6       0.50      0.91      0.65        11\n",
      "           7       0.76      0.73      0.74        22\n",
      "           8       0.52      0.79      0.63        14\n",
      "\n",
      "    accuracy                           0.61       180\n",
      "   macro avg       0.62      0.68      0.61       180\n",
      "weighted avg       0.67      0.61      0.59       180\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "predict=classifier.predict(tv.transform(fenci(x_test)))\n",
    "print(\"每个类别的精确率与召回率\\n\",classification_report(predict, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fbae6c8-44ef-473c-8399-a85292516a2e",
   "metadata": {},
   "source": [
    "## N-gram 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1ae0e946-6426-4e82-b37d-2c17258821c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Software\\Anaconda\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['①①', '①②', '①③', '①④', '①⑤', '①⑥', '①⑦', '①⑧', '①⑨', '①Ａ', '①Ｂ', '①Ｃ', '①Ｄ', '①Ｅ', '①ａ', '①ｃ', '①ｄ', '①ｅ', '①ｆ', '①ｇ', '①ｈ', '①ｉ', '①ｏ', '②①', '②②', '②③', '②④', '②⑤', '②⑥', '②⑦', '②⑧', '②⑩', '②Ｂ', '②Ｇ', '②ａ', '②ｂ', '②ｄ', '②ｅ', '②ｆ', '②ｇ', '②ｈ', '②ｉ', '②ｊ', '③①', '③⑩', '③Ｆ', '③ａ', '③ｂ', '③ｃ', '③ｄ', '③ｅ', '③ｇ', '③ｈ', '④ａ', '④ｂ', '④ｃ', '④ｄ', '④ｅ', '⑤ａ', '⑤ｂ', '⑤ｄ', '⑤ｅ', '⑤ｆ', '１２'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5666666666666667"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tv_2gram = TfidfVectorizer(stop_words=stopwords, max_features=5000, ngram_range=(1,2),lowercase = False)\n",
    "tv_2gram.fit(x_train_fenci)\n",
    "#训练模型\n",
    "clf_2gram = MultinomialNB()\n",
    "clf_2gram.fit(tv_2gram.transform(fenci(x_train)), y_train)\n",
    "#预测\n",
    "clf_2gram.score(tv_2gram.transform(fenci(x_test)), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c28c78ba-57c2-4207-9d97-90f50e8bc0a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "每个类别的精确率与召回率\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.39      0.85      0.54        13\n",
      "           1       0.65      0.71      0.68        21\n",
      "           2       0.86      0.28      0.42        43\n",
      "           3       0.50      0.32      0.39        22\n",
      "           4       0.72      0.57      0.63        23\n",
      "           5       0.52      0.79      0.63        14\n",
      "           6       0.45      0.90      0.60        10\n",
      "           7       0.67      0.67      0.67        21\n",
      "           8       0.48      0.77      0.59        13\n",
      "\n",
      "    accuracy                           0.57       180\n",
      "   macro avg       0.58      0.65      0.57       180\n",
      "weighted avg       0.64      0.57      0.55       180\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "predict=clf_2gram.predict(tv_2gram.transform(fenci(x_test)))\n",
    "print(\"每个类别的精确率与召回率\\n\",classification_report(predict, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148d1be2-0413-427a-a005-16ec95209401",
   "metadata": {},
   "source": [
    "## 3-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fd6dc1bd-7b71-4d90-baf5-159284dd54c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Software\\Anaconda\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['①①', '①②', '①③', '①④', '①⑤', '①⑥', '①⑦', '①⑧', '①⑨', '①Ａ', '①Ｂ', '①Ｃ', '①Ｄ', '①Ｅ', '①ａ', '①ｃ', '①ｄ', '①ｅ', '①ｆ', '①ｇ', '①ｈ', '①ｉ', '①ｏ', '②①', '②②', '②③', '②④', '②⑤', '②⑥', '②⑦', '②⑧', '②⑩', '②Ｂ', '②Ｇ', '②ａ', '②ｂ', '②ｄ', '②ｅ', '②ｆ', '②ｇ', '②ｈ', '②ｉ', '②ｊ', '③①', '③⑩', '③Ｆ', '③ａ', '③ｂ', '③ｃ', '③ｄ', '③ｅ', '③ｇ', '③ｈ', '④ａ', '④ｂ', '④ｃ', '④ｄ', '④ｅ', '⑤ａ', '⑤ｂ', '⑤ｄ', '⑤ｅ', '⑤ｆ', '１２'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5333333333333333"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tv_3gram = TfidfVectorizer(stop_words=stopwords, max_features=5000, ngram_range=(1,3),lowercase = False)\n",
    "tv_3gram.fit(x_train_fenci)\n",
    "#训练模型\n",
    "clf_3gram = MultinomialNB()\n",
    "clf_3gram.fit(tv_3gram.transform(fenci(x_train)), y_train)\n",
    "#预测\n",
    "clf_3gram.score(tv_3gram.transform(fenci(x_test)), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a677781f-da97-4706-9e25-d4aa66a88f24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "每个类别的精确率与召回率\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.66      0.65        56\n",
      "           1       0.80      0.65      0.72        80\n",
      "           2       0.73      0.79      0.76        68\n",
      "           3       0.67      0.65      0.66        65\n",
      "           4       0.76      0.80      0.78        70\n",
      "           5       0.74      0.82      0.78        68\n",
      "           6       0.54      0.75      0.63        53\n",
      "           7       0.76      0.41      0.54        99\n",
      "           8       0.57      0.86      0.69        42\n",
      "\n",
      "    accuracy                           0.69       601\n",
      "   macro avg       0.69      0.71      0.69       601\n",
      "weighted avg       0.70      0.69      0.68       601\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "predict=clf_3gram.predict(tv_3gram.transform(fenci(x_test)))\n",
    "print(\"每个类别的精确率与召回率\\n\",classification_report(predict, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493c8360-1db2-4f32-8f49-f7f5d906cb40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
